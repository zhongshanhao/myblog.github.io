<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>python机器学习分类算法--自适应线性神经网络（Adaline）</title>
      <link href="/2019/10/05/AdalineGd/"/>
      <url>/2019/10/05/AdalineGd/</url>
      
        <content type="html"><![CDATA[<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>定义类AdalineGD封装分类算法，调用fit方法拟合数据，训练模型；调用predict方法测试模型，返回分类类标（1，-1）。</p><p>定义plot_decision_regions类，将分类结果可视化。</p><pre><code class="python">import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom matplotlib.colors import ListedColormapclass AdalineGD(object):    def __init__(self, eta=0.01, n_iter=50, random_state=1):                self.eta = eta                self.n_iter = n_iter                self.random_state = random_state    def fit(self, X, y):        rgen = np.random.RandomState(self.random_state)                self.w_ = rgen.normal(loc=0.0, scale=0.01,size=1 + X.shape[1])                self.cost_ = []        for i in range(self.n_iter):                        net_input = self.net_input(X)                        output = self.activation(net_input)                        errors = (y - output)                        self.w_[1:] += self.eta * X.T.dot(errors)                        self.w_[0] += self.eta * errors.sum()                        cost = (errors**2).sum() / 2.0                        self.cost_.append(cost)                return self    def net_input(self, X):                &quot;&quot;&quot;Calculate net input&quot;&quot;&quot;                return np.dot(X, self.w_[1:]) + self.w_[0]    def activation(self,X):        return X    def predict(self, X):                &quot;&quot;&quot;Return class label after unit step&quot;&quot;&quot;                return np.where(self.activation(self.net_input(X)) &gt;= 0.0, 1, -1)def plot_decision_regions(X, y, classifier, resolution=0.02):    # setup marker generator and color map        markers = (&#39;s&#39;, &#39;x&#39;, &#39;o&#39;, &#39;^&#39;, &#39;v&#39;)        colors = (&#39;red&#39;, &#39;blue&#39;, &#39;lightgreen&#39;, &#39;gray&#39;, &#39;cyan&#39;)        cmap = ListedColormap(colors[:len(np.unique(y))])    # plot the decision surface        x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1        x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1        xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),                                                      np.arange(x2_min, x2_max, resolution))        Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)        Z = Z.reshape(xx1.shape)        plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)        plt.xlim(xx1.min(), xx1.max())        plt.ylim(xx2.min(), xx2.max())    # plot class samples        for idx, cl in enumerate(np.unique(y)):                plt.scatter(x=X[y == cl, 0],                                        y=X[y == cl, 1],                                        alpha=0.8,                                        c=colors[idx],                                        marker=markers[idx],                                        label=cl,                                        edgecolor=&#39;black&#39;)</code></pre><p>测试选用不同学习率0.01和0.001，可以从下图看到，选用学习率0.01使得代价函数随着迭代次数的增加而增加，而选用学习率0.001使得代价函数逐渐收敛。在实际解决问题的过程中，选择过大的学习率可能会错过全局最优解，我们应该选择合适的学习率。</p><pre><code class="python">df = pd.read_csv(&#39;https://archive.ics.uci.edu/ml/&#39; ...              &#39;machine-learning-databases/iris/iris.data&#39;, ...               header=None)# select setosa and versicolor y = df.iloc[0:100, 4].values y = np.where(y == &#39;Iris-setosa&#39;, -1, 1)# extract sepal length and petal length X = df.iloc[0:100, [0, 2]].valuesfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))ada1 = AdalineGD(n_iter=10, eta=0.01).fit(X, y)ax[0].plot(range(1, len(ada1.cost_) + 1),np.log10(ada1.cost_), marker=&#39;o&#39;)ax[0].set_xlabel(&#39;Epochs&#39;)ax[0].set_ylabel(&#39;log(Sum-squared-error)&#39;)ax[0].set_title(&#39;Adaline - Learning rate 0.01&#39;)ada2 = AdalineGD(n_iter=10, eta=0.0001).fit(X, y)ax[1].plot(range(1, len(ada2.cost_) + 1),ada2.cost_, marker=&#39;o&#39;)ax[1].set_xlabel(&#39;Epochs&#39;)ax[1].set_ylabel(&#39;Sum-squared-error&#39;)ax[1].set_title(&#39;Adaline - Learning rate 0.0001&#39;)plt.show()</code></pre><p><img src="http://i1.fuimg.com/700703/e8188b7c3918646c.png" alt></p><p>为了优化算法性能，可以对数据做特征缩放，采用以下规则对数据进行特征缩放：<br>$$<br>x_{j}:=\frac{x_j-u_j}{\delta_j}<br>$$<br>$x_j$为训练样本n中第j个特征的所有值的向量，$u_j$、$\delta_j$分别是样本中第j个特征的平均值和标准差。</p><pre><code class="python">X_std = np.copy(X)X_std[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()X_std[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()ada = AdalineGD(n_iter=15, eta=0.01)ada.fit(X_std, y)plot_decision_regions(X_std, y, classifier=ada)plt.title(&#39;Adaline - Gradient Descent&#39;) plt.xlabel(&#39;sepal length [standardized]&#39;)plt.ylabel(&#39;petal length [standardized]&#39;)plt.legend(loc=&#39;upper left&#39;)plt.tight_layout()plt.show()plt.plot(range(1, len(ada.cost_) + 1), ada.cost_, marker=&#39;o&#39;)plt.xlabel(&#39;Epochs&#39;)plt.ylabel(&#39;Sum-squared-error&#39;)plt.show()</code></pre><p><img src="http://i1.fuimg.com/700703/cf61ff26b7cd2770.png" alt></p><p><img src="http://i1.fuimg.com/700703/ae4e566acf916b3d.png" alt></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>python机器学习分类算法--感知器</title>
      <link href="/2019/10/03/ganzhiji/"/>
      <url>/2019/10/03/ganzhiji/</url>
      
        <content type="html"><![CDATA[<h2 id="感知器基本原理"><a href="#感知器基本原理" class="headerlink" title="感知器基本原理"></a>感知器基本原理</h2><p>定义输入向量x，权值向量w，z是x与w的线性组合，z称作为净输入。</p><p>$$<br>x=\begin{bmatrix}x_{0}<br>\\ x_{1}<br>\\ …<br>\\ x_{m}<br>\end{bmatrix},x_{0}=1,w = \begin{bmatrix}w_{0}<br>\\ w_{1}<br>\\ …<br>\\ w_{m}<br>\end{bmatrix},z = x_{0}w_{0} + x_{1}w_{1} + … + x_{m}w_{m} = x^{T}w,<br>$$</p><p>定义激励函数h(z)，激励函数是一个分段函数，简单来说，当净输入z大于0时，将其划分到1类，否则为 -1类。</p><p>$$<br>h(z) = \begin{cases}<br>1 &amp; \text{ if } z\geq 0 \\<br>-1 &amp; \text{ if } z= else<br>\end{cases}<br>$$</p><p>感知器将输入值乘以权值得到净输入，通过激励函数将样本分为正负两类，感知器的工作过程如下：</p><ol><li>将权重初始化为零或一个极小的随机数。</li><li>迭代所有的训练样本x(i),执行以下操作：</li></ol><p>​    (1)计算输出值$\hat{y} $</p><p>​    (2)更新权重$w_{j} $<br>$$<br>w_{j}:=w_{j}+\Delta w_{j}\\ \Delta w_{j}=\eta(y^{(i)}-\hat{y}^{(i)})x_{j}^{(i)}<br>$$<br>$\eta$为学习率，0~1之间的数。</p><p>通过给每一个特征$x_i$分配对应权重$w_i$，将对应特征和权重相乘后求和得到$z$，然后根据激励函数将z划分为两类，若通过激励函数后所得的输出值（即预测值）与训练数据对应$y_i$相符合，则不改变权重w，若不相符，则改变权重w，使得重新分配的权重在下次分类更接近或者达到真实值。</p><p>需要注意的是，若样本线性不可分，权重w会不断更新，在实际代码实现的时候需要设定一个最大迭代次数。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>定义类Perceptron，创建对象时可选参数有eta即学习率$\eta$，n_iter为迭代次数，可调用fit方法训练模型，调用predict方法测试模型。</p><pre><code>import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom matplotlib.colors import ListedColormapclass Perceptron(object):    def __init__(self, eta=0.01, n_iter=50, random_state=1):                self.eta = eta                self.n_iter = n_iter                self.random_state = random_state    def fit(self, X, y):        rgen = np.random.RandomState(self.random_state)                self.w_ = rgen.normal(loc=0.0, scale=0.01,size=1 + X.shape[1])                self.errors_ = []        for _ in range(self.n_iter):                        errors = 0                        for xi, target in zip(X, y):                                update = self.eta * (target - self.predict(xi))                                self.w_[1:] += update * xi                                self.w_[0] += update                                errors += int(update != 0.0)                        self.errors_.append(errors)                return self    def net_input(self, X):                &quot;&quot;&quot;Calculate net input&quot;&quot;&quot;                return np.dot(X, self.w_[1:]) + self.w_[0]    def predict(self, X):                &quot;&quot;&quot;Return class label after unit step&quot;&quot;&quot;                return np.where(self.net_input(X) &gt;= 0.0, 1, -1)</code></pre><p>从pd库中调用数据集，下表为数据集的最后5条数据。</p><pre><code class="python">df = pd.read_csv(&#39;https://archive.ics.uci.edu/ml/&#39; ...              &#39;machine-learning-databases/iris/iris.data&#39;, ...               header=None) df.tail()</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th {    vertical-align: middle;}.dataframe thead th {    text-align: right;}</code></pre><p></style><p></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>0</th>      <th>1</th>      <th>2</th>      <th>3</th>      <th>4</th>    </tr>  </thead>  <tbody>    <tr>      <th>145</th>      <td>6.7</td>      <td>3.0</td>      <td>5.2</td>      <td>2.3</td>      <td>Iris-virginica</td>    </tr>    <tr>      <th>146</th>      <td>6.3</td>      <td>2.5</td>      <td>5.0</td>      <td>1.9</td>      <td>Iris-virginica</td>    </tr>    <tr>      <th>147</th>      <td>6.5</td>      <td>3.0</td>      <td>5.2</td>      <td>2.0</td>      <td>Iris-virginica</td>    </tr>    <tr>      <th>148</th>      <td>6.2</td>      <td>3.4</td>      <td>5.4</td>      <td>2.3</td>      <td>Iris-virginica</td>    </tr>    <tr>      <th>149</th>      <td>5.9</td>      <td>3.0</td>      <td>5.1</td>      <td>1.8</td>      <td>Iris-virginica</td>    </tr>  </tbody></table></div><p>使用matplotlib库将数据可视化。</p><pre><code class="python">import matplotlib.pyplot as plt# select setosa and versicolor y = df.iloc[0:100, 4].values y = np.where(y == &#39;Iris-setosa&#39;, -1, 1)# extract sepal length and petal length X = df.iloc[0:100, [0, 2]].values# plot data plt.scatter(X[:50, 0], X[:50, 1], label=&#39;setosa&#39;) plt.scatter(X[50:100, 0], X[50:100, 1],marker=&#39;x&#39;,label=&#39;versicolor&#39;)  plt.xlabel(&#39;sepal length [cm]&#39;) plt.ylabel(&#39;petal length [cm]&#39;)plt.legend(loc=&#39;upper left&#39;) plt.show() </code></pre><p><img src="http://i2.tiimg.com/700703/d147555495de8c6b.png" alt="散点图"></p><p>迭代次数和预测错误次数的折线图。</p><pre><code class="python">ppn = Perceptron(eta=0.1,n_iter=10)ppn.fit(X, y)plt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker=&#39;o&#39;) plt.xlabel(&#39;Epochs&#39;)plt.ylabel(&#39;Number of updates&#39;)plt.show()</code></pre><p><img src="http://i2.tiimg.com/700703/0f0a93745808decc.png" alt="折线图"></p><pre><code class="python">def plot_decision_regions(X, y, classifier, resolution=0.02):    # setup marker generator and color map        markers = (&#39;s&#39;, &#39;x&#39;, &#39;o&#39;, &#39;^&#39;, &#39;v&#39;)        colors = (&#39;red&#39;, &#39;blue&#39;, &#39;lightgreen&#39;, &#39;gray&#39;, &#39;cyan&#39;)        cmap = ListedColormap(colors[:len(np.unique(y))])    # plot the decision surface        x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1        x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1        xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),                                                      np.arange(x2_min, x2_max, resolution))        Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)        Z = Z.reshape(xx1.shape)        plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)        plt.xlim(xx1.min(), xx1.max())        plt.ylim(xx2.min(), xx2.max())    # plot class samples        for idx, cl in enumerate(np.unique(y)):                plt.scatter(x=X[y == cl, 0],                                        y=X[y == cl, 1],                                        alpha=0.8,                                        c=colors[idx],                                        marker=markers[idx],                                        label=cl,                                        edgecolor=&#39;black&#39;)plot_decision_regions(X, y, classifier=ppn)plt.xlabel(&#39;sepal length [cm]&#39;)plt.ylabel(&#39;petal length [cm]&#39;)plt.legend(loc=&#39;upper left&#39;)plt.show()</code></pre><p><img src="http://i2.tiimg.com/700703/15a977e327e47b82.png" alt="分类图"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>numpy_矩阵切片</title>
      <link href="/2019/10/03/numpy-array/"/>
      <url>/2019/10/03/numpy-array/</url>
      
        <content type="html"><![CDATA[<p>和python列表一样，numpy库中array也有着可切片的特性，这大大方便了我们对矩阵的可操作性。</p><pre><code class="python">import numpy as npa = np.arange(12)a.shape = (3,4)   #同写法a = a.reshape(3,4)print(a)</code></pre><p>首先我们现创建一个3*4的矩阵，如下：</p><pre><code>[[ 0  1  2  3] [ 4  5  6  7] [ 8  9 10 11]]</code></pre><p>可通过a[x,y]选择第x+1行第y+1列的元素</p><pre><code class="python">print(a[2,3]) #选择矩阵第3行第4列</code></pre><pre><code>11</code></pre><p>通过以下方法可对矩阵进行切割。</p><p>a[:,y]中“：”表示选择所有行，y表示选择第y+1列，a[:,1]表示选择矩阵a第2列的所有行，也就是[1 5 9]；</p><p>a[1,:]表示选择矩阵a第2行的所有列，也就是[4 5 6 7]。</p><p>a[:,:2]表示选择矩阵a第1和第2列的所有行。</p><pre><code class="python">print(a[:,1])  #选择矩阵第二列 print(a[1,:])  #选择矩阵第二行  print(a[:,:2]) #选择矩阵第一和第二列</code></pre><pre><code>[1 5 9][4 5 6 7][[0 1] [4 5] [8 9]]</code></pre><p>选择矩阵第2，3行和第2，3列</p><pre><code>print(a[1:,1:3]) </code></pre><pre><code>[[ 5  6] [ 9 10]]</code></pre><pre><code>print(a[:,[0,2]) #选择矩阵a的第1和第3列[[0 2] [4 6] [8 10]]</code></pre><p>用ravel()方法可将矩阵按行展开</p><pre><code class="python">print(a.ravel())#将矩阵转换成列表</code></pre><pre><code>[ 0  1  2  3  4  5  6  7  8  9 10 11]</code></pre><p>方法np.where(condition,x1,x2)，矩阵元素满足condition则选择用x1替换，否则用x2替换</p><pre><code class="python">y = np.array([&#39;one&#39;,&#39;zreo&#39;,&#39;one&#39;])y = np.where(y == &#39;one&#39;,1,0) #满足条件选1，不满足选0print(y)</code></pre><pre><code>[1 0 1]</code></pre>]]></content>
      
      
      <categories>
          
          <category> numpy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> numpy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习概述</title>
      <link href="/2019/10/02/intro-machinelearning/"/>
      <url>/2019/10/02/intro-machinelearning/</url>
      
        <content type="html"><![CDATA[<h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><p>近年来，NumPy、matplotlib和pandas等多种功能强大的开源库的出现，以及利用深度学习框架tensorflow等，使得机器学习这一领域绽放了无限的生机。</p><h3 id="机器学习的三种方法"><a href="#机器学习的三种方法" class="headerlink" title="机器学习的三种方法"></a>机器学习的三种方法</h3><ol><li><p>监督学习(supervised learning)</p><p>监督学习处理的问题分为两种，一是分类问题（classification），根据已经标记有是否为垃圾邮件的样本库，判断一封新邮件是否为垃圾邮件的问题。二是回归问题（regression），如我们想预测学生在SAT考试中数学科目的成绩，根据往期学生成绩与学习时间建立训练模型，通过学习时间预测学生的数学成绩。</p><p>监督学习是根据已有的样本训练模型，然后对未知的样本进行预测的一种方法。</p></li><li><p>非监督学习(unsupervised learning)</p><p>无监督学习是在没有已知输出变量和反馈函数指导的情况下提取有效信息来探索数据的整体结构的。通过非监督学习能够发现数据本身潜在的结构，在数据压缩中的降维邻域非监督学习也发挥着重要作用。</p></li><li><p>强化学习(reinforcement learning)</p><p>强化学习构建一个系统，在与环境的交互的过程中提高系统的性能。一个强化学习的经典例子就是象棋对弈游戏，系统根据当前局态（环境）决定落子的位置，游戏结束时胜负的判定可以作为反馈，通过这个反馈让系统做出调整，优化系统，提升系统的性能。</p></li></ol><h3 id="选择不同的算法训练模型"><a href="#选择不同的算法训练模型" class="headerlink" title="选择不同的算法训练模型"></a>选择不同的算法训练模型</h3><p>任何分类算法都有内在的局限性，在实际解决问题的过程中， 应该选用几种不同的算法来训练模型，并比较它们的性能，从中选择最优的一个。一个常用的性能指标是分类的准确性。</p><h3 id="机器学习技能树"><a href="#机器学习技能树" class="headerlink" title="机器学习技能树"></a>机器学习技能树</h3><p><img src="http://i1.fuimg.com/700703/cf43e5cf096624cd.png" alt="机器学习技能图谱"></p><p>图谱从<a href="https://github.com/TeamStuQ/skill-map转载" target="_blank" rel="noopener">https://github.com/TeamStuQ/skill-map转载</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>写在博客之前</title>
      <link href="/2019/10/02/whyblog/"/>
      <url>/2019/10/02/whyblog/</url>
      
        <content type="html"><![CDATA[<h2 id="Why-Blog？"><a href="#Why-Blog？" class="headerlink" title="Why Blog？"></a>Why Blog？</h2><blockquote><p>记录日常学习笔记。</p><p>方便快速复习已经学过的知识。</p><p>增加写文能力。</p><p>希望能帮助到其他人。</p></blockquote><h2 id="工匠精神"><a href="#工匠精神" class="headerlink" title="工匠精神"></a>工匠精神</h2><p>当今社会，经济和技术高速发展，身处于这样一个时代的我们难免有些浮躁。我所推崇的”工匠精神“，是一种刻骨钻研、追求创新的精神。我所希望的是，任何一种职业，不管在学习还是工作中，都应该像一名工匠一样，尽自己所能把事情做好，做精。</p>]]></content>
      
      
      <categories>
          
          <category> 杂谈 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 其他 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
