<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>python机器学习-数据预处理</title>
      <link href="/2019/10/10/data-preprocessing/"/>
      <url>/2019/10/10/data-preprocessing/</url>
      
        <content type="html"><![CDATA[<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><h3 id="处理非数值型数据"><a href="#处理非数值型数据" class="headerlink" title="处理非数值型数据"></a>处理非数值型数据</h3><pre><code class="python">import pandas as pd  df = pd.DataFrame([                     [&#39;green&#39;, &#39;M&#39;, 10.1, &#39;class1&#39;],                     [&#39;red&#39;, &#39;L&#39;, 13.5, &#39;class2&#39;],                     [&#39;blue&#39;, &#39;XL&#39;, 15.3, &#39;class1&#39;]]) df.columns = [&#39;color&#39;, &#39;size&#39;, &#39;price&#39;, &#39;classlabel&#39;] print(df)</code></pre><pre><code>   color size  price classlabel0  green    M   10.1     class11    red    L   13.5     class22   blue   XL   15.3     class1</code></pre><p>利用pandas将CSV格式的数据读取进来，要将字符串转换为适合机器学习算法训练的数据类型。<br>先对特征size数据进行处理，由于XL&gt;L&gt;M，size特征有这样的一个顺序特征，所以将这类字符串转换为数值时也该保留顺序特征。利用映射函数map很容易将字符串转换：</p><pre><code class="python">size_mapping = {                                  &#39;XL&#39;: 3,                                  &#39;L&#39;: 2,                                &#39;M&#39;: 1} df[&#39;size&#39;] = df[&#39;size&#39;].map(size_mapping) print(df)</code></pre><pre><code>   color  size  price classlabel0  green     1   10.1     class11    red     2   13.5     class22   blue     3   15.3     class1</code></pre><p>可以利用小技巧将数值数据转换回字符串：</p><pre><code class="python">inv_size_mapping = {v: k for k, v in size_mapping.items()}print(df[&#39;size&#39;].map(inv_size_mapping) )</code></pre><pre><code>0     M1     L2    XLName: size, dtype: object</code></pre><p>对于分类标签classlabe，只需要将字符串转换为数值特征就好。<br>利用枚举函数enumerate()和np.unique()将标签导出为映射字典，在用映射函数map将分类标签转换为整数：</p><pre><code class="python">import numpy as npclass_mapping = {label:idx for idx,label in                                  enumerate(np.unique(df[&#39;classlabel&#39;]))}df[&#39;classlabel&#39;] = df[&#39;classlabel&#39;].map(class_mapping) print(df)</code></pre><pre><code>   color  size  price  classlabel0  green     1   10.1           01    red     2   13.5           12   blue     3   15.3           0</code></pre><p>同样将整数值可以反映射回分类标签：</p><pre><code class="python">inv_class_mapping = {v: k for k, v in class_mapping.items()}df[&#39;classlabel&#39;] = df[&#39;classlabel&#39;].map(inv_class_mapping) print(df)</code></pre><pre><code>   color  size  price classlabel0  green     1   10.1     class11    red     2   13.5     class22   blue     3   15.3     class1</code></pre><p>也可直接调用sklearn库LabelEncoder类实现：</p><pre><code class="python">from sklearn.preprocessing import LabelEncoder class_le = LabelEncoder() y = class_le.fit_transform(df[&#39;classlabel&#39;].values) print(y)</code></pre><pre><code>[0 1 0]</code></pre><pre><code class="python">class_le.inverse_transform(y)</code></pre><pre><code>array([&#39;class1&#39;, &#39;class2&#39;, &#39;class1&#39;], dtype=object)</code></pre><p>接下来考虑特征color，颜色green，red，blue没有顺序特征，可以采用热编码的方式将其转换，调用pandas中的get_dummies方法转换字符串：</p><pre><code class="python">pd.get_dummies(df[[&#39;price&#39;, &#39;color&#39;, &#39;size&#39;]])</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th {    vertical-align: top;}.dataframe thead th {    text-align: right;}</code></pre><p></style><p></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>price</th>      <th>size</th>      <th>color_blue</th>      <th>color_green</th>      <th>color_red</th>    </tr>  </thead>  <tbody>    <tr>      <td>0</td>      <td>10.1</td>      <td>1</td>      <td>0</td>      <td>1</td>      <td>0</td>    </tr>    <tr>      <td>1</td>      <td>13.5</td>      <td>2</td>      <td>0</td>      <td>0</td>      <td>1</td>    </tr>    <tr>      <td>2</td>      <td>15.3</td>      <td>3</td>      <td>1</td>      <td>0</td>      <td>0</td>    </tr>  </tbody></table></div><h3 id="划分数据集为训练集和测试集"><a href="#划分数据集为训练集和测试集" class="headerlink" title="划分数据集为训练集和测试集"></a>划分数据集为训练集和测试集</h3><pre><code class="python">import pandas as pdimport numpy as npdf_wine = pd.read_csv(&#39;https://archive.ics.uci.edu/&#39;                                                 &#39;ml/machine-learning-databases/&#39;                                                 &#39;wine/wine.data&#39;, header=None) df_wine.columns = [&#39;Class label&#39;, &#39;Alcohol&#39;,                                         &#39;Malic acid&#39;, &#39;Ash&#39;,                                       &#39;Alcalinity of ash&#39;, &#39;Magnesium&#39;,                                        &#39;Total phenols&#39;, &#39;Flavanoids&#39;,                                           &#39;Nonflavanoid phenols&#39;,                                           &#39;Proanthocyanins&#39;,                                            &#39;Color intensity&#39;, &#39;Hue&#39;,                                            &#39;OD280/OD315 of diluted wines&#39;,                                            &#39;Proline&#39;]  print(&#39;Class labels&#39;, np.unique(df_wine[&#39;Class label&#39;])) df_wine.head()</code></pre><pre><code>Class labels [1 2 3]</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th {    vertical-align: top;}.dataframe thead th {    text-align: right;}</code></pre><p></style><p></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Class label</th>      <th>Alcohol</th>      <th>Malic acid</th>      <th>Ash</th>      <th>Alcalinity of ash</th>      <th>Magnesium</th>      <th>Total phenols</th>      <th>Flavanoids</th>      <th>Nonflavanoid phenols</th>      <th>Proanthocyanins</th>      <th>Color intensity</th>      <th>Hue</th>      <th>OD280/OD315 of diluted wines</th>      <th>Proline</th>    </tr>  </thead>  <tbody>    <tr>      <td>0</td>      <td>1</td>      <td>14.23</td>      <td>1.71</td>      <td>2.43</td>      <td>15.6</td>      <td>127</td>      <td>2.80</td>      <td>3.06</td>      <td>0.28</td>      <td>2.29</td>      <td>5.64</td>      <td>1.04</td>      <td>3.92</td>      <td>1065</td>    </tr>    <tr>      <td>1</td>      <td>1</td>      <td>13.20</td>      <td>1.78</td>      <td>2.14</td>      <td>11.2</td>      <td>100</td>      <td>2.65</td>      <td>2.76</td>      <td>0.26</td>      <td>1.28</td>      <td>4.38</td>      <td>1.05</td>      <td>3.40</td>      <td>1050</td>    </tr>    <tr>      <td>2</td>      <td>1</td>      <td>13.16</td>      <td>2.36</td>      <td>2.67</td>      <td>18.6</td>      <td>101</td>      <td>2.80</td>      <td>3.24</td>      <td>0.30</td>      <td>2.81</td>      <td>5.68</td>      <td>1.03</td>      <td>3.17</td>      <td>1185</td>    </tr>    <tr>      <td>3</td>      <td>1</td>      <td>14.37</td>      <td>1.95</td>      <td>2.50</td>      <td>16.8</td>      <td>113</td>      <td>3.85</td>      <td>3.49</td>      <td>0.24</td>      <td>2.18</td>      <td>7.80</td>      <td>0.86</td>      <td>3.45</td>      <td>1480</td>    </tr>    <tr>      <td>4</td>      <td>1</td>      <td>13.24</td>      <td>2.59</td>      <td>2.87</td>      <td>21.0</td>      <td>118</td>      <td>2.80</td>      <td>2.69</td>      <td>0.39</td>      <td>1.82</td>      <td>4.32</td>      <td>1.04</td>      <td>2.93</td>      <td>735</td>    </tr>  </tbody></table></div><p>首先将数据读取，调用train_test_split函数将数据集划分，参数test_size=0.3表示将数据集的30%<br>划分给测试集，把70%划分给训练集，stratify=y表示训练集中各个类别的比例和测试集中一样。</p><pre><code class="python">from sklearn.model_selection import train_test_split X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values X_train, X_test, y_train, y_test =train_test_split(X, y,                                                                   test_size=0.3,                                                                   random_state=0,                                                                  stratify=y)</code></pre><h3 id="数据正则化"><a href="#数据正则化" class="headerlink" title="数据正则化"></a>数据正则化</h3><p>数据正则化对线性模型和梯度下降等优化算法特别有用。但基于树的模型不需要进行标准化。<br>创建StandardScaler对象，调用fit方法获取样本均值和标准差，再用这些参数去转换测试集。</p><pre><code class="python">from sklearn.preprocessing import StandardScaler sc = StandardScaler() sc.fit(X_train) X_train_std = sc.transform(X_train)X_test_std = sc.transform(X_test) </code></pre><h3 id="特征的选择"><a href="#特征的选择" class="headerlink" title="特征的选择"></a>特征的选择</h3><p>L1正则化：</p><pre><code class="python">import matplotlib.pyplot as pltfrom sklearn.linear_model import LogisticRegressionfig = plt.figure() ax = plt.subplot(111)colors = [&#39;blue&#39;, &#39;green&#39;, &#39;red&#39;, &#39;cyan&#39;,           &#39;magenta&#39;, &#39;yellow&#39;, &#39;black&#39;,           &#39;pink&#39;, &#39;lightgreen&#39;, &#39;lightblue&#39;,           &#39;gray&#39;, &#39;indigo&#39;, &#39;orange&#39;] weights, params = [], []for c in np.arange(-4., 6.):     lr = LogisticRegression(penalty=&#39;l1&#39;,                             C=10.**c,                             random_state=0,solver=&#39;liblinear&#39;,multi_class=&#39;ovr&#39;)     lr.fit(X_train_std, y_train)    weights.append(lr.coef_[1])    params.append(10**c)weights = np.array(weights)for column, color in zip(range(weights.shape[1]), colors):    plt.plot(params, weights[:, column],             label=df_wine.columns[column + 1],             color=color) plt.axhline(0, color=&#39;black&#39;, linestyle=&#39;--&#39;, linewidth=3)plt.xlim([10**(-5), 10**5])plt.ylabel(&#39;weight coefficient&#39;)plt.xlabel(&#39;C&#39;) plt.xscale(&#39;log&#39;)plt.legend(loc=&#39;upper left&#39;)ax.legend(loc=&#39;upper center&#39;,          bbox_to_anchor=(1.38, 1.03),          ncol=1, fancybox=True)plt.show() </code></pre><p><img src="http://i1.fuimg.com/700703/62a07faa3d644d63.png" alt="Markdown"></p><p>参数C是正则化参数的逆，参数C越小，模型的正则化强度就越强，上述采用L1正则化，可以看到，当C极小的时候，各个特征的权值都趋近于0，然而，当C越大时，有些特征的权重依然徘徊在0附近，这说明对于该分类模型来说，这些特征对于分类的作用没有那么明显，所以，我们可以在训练模型的时候将这些模型去掉，以简化分类模型，减少泛化误差。可以说，L1正则化是一种特征选择技术。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>git常用命令</title>
      <link href="/2019/10/06/git-command/"/>
      <url>/2019/10/06/git-command/</url>
      
        <content type="html"><![CDATA[<h2 id="创建仓库"><a href="#创建仓库" class="headerlink" title="创建仓库"></a>创建仓库</h2><ol><li>mkdir learngit  创建文件夹</li><li>cd learngit      </li><li>git init        将当前所在的目录变成git可以管理的仓库</li></ol><h2 id="将文件加入git管理仓库"><a href="#将文件加入git管理仓库" class="headerlink" title="将文件加入git管理仓库"></a>将文件加入git管理仓库</h2><ol><li>git add readme.txt 将文件存入git暂存区</li><li>git commit -m “wrote a readme file”   将文件提交给git仓库</li></ol><h2 id="工作区"><a href="#工作区" class="headerlink" title="工作区"></a>工作区</h2><ol><li>git status   查看工作区状态</li><li>Git diff readme.txt 查看工作区和git仓库中文件的不同</li></ol><h2 id="版本回退"><a href="#版本回退" class="headerlink" title="版本回退"></a>版本回退</h2><ol><li>git log –pretty=oneline 查看提交日志</li><li>git reset –hard HEAD^ 版本回退到上一个提交版本 HEAD为指向当前版本指针</li><li>git reset –hard 1094a 版本回到特定版本号，可用git log命令查看对应的版本号</li></ol><h2 id="删除文件"><a href="#删除文件" class="headerlink" title="删除文件"></a>删除文件</h2><ol><li>rm readme.txt  在工作区删除文件</li><li>git rm readme.txt 在git仓库删除文件</li><li>git checkout – readme.txt 从git仓库中恢复文件到工作区 前提git仓库有readme.txt</li></ol><h2 id="添加远程库"><a href="#添加远程库" class="headerlink" title="添加远程库"></a>添加远程库</h2><ol><li>git remote add origin <a href="mailto:git@github.com" target="_blank" rel="noopener">git@github.com</a>:zhongshanhao/learngit.git 关联远程库</li><li>git push -u origin master 第一次推送master分支的所有内容</li><li>git push origin master 推送master分支的所有内容</li></ol><h2 id="克隆远程库"><a href="#克隆远程库" class="headerlink" title="克隆远程库"></a>克隆远程库</h2><p>1.git clone <a href="mailto:git@github.com" target="_blank" rel="noopener">git@github.com</a>:zhongshanhao/learngit.git</p><h2 id="查看当前连接的远程库"><a href="#查看当前连接的远程库" class="headerlink" title="查看当前连接的远程库"></a>查看当前连接的远程库</h2><p>1.git remote -v</p><h2 id="本地同步远程仓库"><a href="#本地同步远程仓库" class="headerlink" title="本地同步远程仓库"></a>本地同步远程仓库</h2><p>1.git pull        </p><p>2.git remote set-url origin <a href="https://github.com/zhongshanhao/python.git" target="_blank" rel="noopener">https://github.com/zhongshanhao/python.git</a></p><p>3.git pull origin master –allow-unrelated-histories //把远程仓库和本地同步，消除差异 </p>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>python机器学习分类算法--自适应线性神经网络（Adaline）</title>
      <link href="/2019/10/05/AdalineGd/"/>
      <url>/2019/10/05/AdalineGd/</url>
      
        <content type="html"><![CDATA[<h2 id="算法初步"><a href="#算法初步" class="headerlink" title="算法初步"></a>算法初步</h2><p>Adaline可以看作是对感知器算法的优化和改进，Adaline算法定义了最小化连续性代价函数的概念，这为理解如逻辑回归、支持向量机和回归模型等更高级的机器学习算法奠定了基础。</p><p>定义代价函数：<br>$$<br>J(w) = \frac{1}{2}\sum_{i=1}^{m}(y^{(i)}-h_w(x^{(i)}))^2<br>\\ h_w(x) = w^Tx<br>$$<br>我们要优化目标函数$h_w(x)$，使得输出值符合实际值，就要尽可能降低代价函数$J(w)$,找到使得代价函数最小的w。</p><p>利用梯度下降求最优解：<br>$$<br>w_j:=w_j-\Delta w_j \<br> \Delta w_j = \eta \frac{\delta J}{\delta w_j}=\eta \sum_{i=1}^{m}(h_w(x^{(i)})-y^{(i)})x_j^{(i)}<br>$$</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>定义类AdalineGD封装分类算法，调用fit方法拟合数据，训练模型；调用predict方法测试模型，返回分类类标（1，-1）。</p><p>定义plot_decision_regions类，将分类结果可视化。</p><pre><code class="python">import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom matplotlib.colors import ListedColormapclass AdalineGD(object):    def __init__(self, eta=0.01, n_iter=50, random_state=1):                self.eta = eta                self.n_iter = n_iter                self.random_state = random_state    def fit(self, X, y):        rgen = np.random.RandomState(self.random_state)                self.w_ = rgen.normal(loc=0.0, scale=0.01,size=1 + X.shape[1])                self.cost_ = []        for i in range(self.n_iter):                        net_input = self.net_input(X)                        output = self.activation(net_input)                        errors = (y - output)                        self.w_[1:] += self.eta * X.T.dot(errors)                        self.w_[0] += self.eta * errors.sum()                        cost = (errors**2).sum() / 2.0                        self.cost_.append(cost)                return self    def net_input(self, X):                &quot;&quot;&quot;Calculate net input&quot;&quot;&quot;                return np.dot(X, self.w_[1:]) + self.w_[0]    def activation(self,X):        return X    def predict(self, X):                &quot;&quot;&quot;Return class label after unit step&quot;&quot;&quot;                return np.where(self.activation(self.net_input(X)) &gt;= 0.0, 1, -1)def plot_decision_regions(X, y, classifier, resolution=0.02):    # setup marker generator and color map        markers = (&#39;s&#39;, &#39;x&#39;, &#39;o&#39;, &#39;^&#39;, &#39;v&#39;)        colors = (&#39;red&#39;, &#39;blue&#39;, &#39;lightgreen&#39;, &#39;gray&#39;, &#39;cyan&#39;)        cmap = ListedColormap(colors[:len(np.unique(y))])    # plot the decision surface        x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1        x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1        xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),                                                      np.arange(x2_min, x2_max, resolution))        Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)        Z = Z.reshape(xx1.shape)        plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)        plt.xlim(xx1.min(), xx1.max())        plt.ylim(xx2.min(), xx2.max())    # plot class samples        for idx, cl in enumerate(np.unique(y)):                plt.scatter(x=X[y == cl, 0],                                        y=X[y == cl, 1],                                        alpha=0.8,                                        c=colors[idx],                                        marker=markers[idx],                                        label=cl,                                        edgecolor=&#39;black&#39;)</code></pre><p>测试选用不同学习率0.01和0.001，可以从下图看到，选用学习率0.01使得代价函数随着迭代次数的增加而增加，而选用学习率0.001使得代价函数逐渐收敛。在实际解决问题的过程中，选择过大的学习率可能会错过全局最优解，我们应该选择合适的学习率。</p><pre><code class="python">df = pd.read_csv(&#39;https://archive.ics.uci.edu/ml/&#39; ...              &#39;machine-learning-databases/iris/iris.data&#39;, ...               header=None)# select setosa and versicolor y = df.iloc[0:100, 4].values y = np.where(y == &#39;Iris-setosa&#39;, -1, 1)# extract sepal length and petal length X = df.iloc[0:100, [0, 2]].valuesfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))ada1 = AdalineGD(n_iter=10, eta=0.01).fit(X, y)ax[0].plot(range(1, len(ada1.cost_) + 1),np.log10(ada1.cost_), marker=&#39;o&#39;)ax[0].set_xlabel(&#39;Epochs&#39;)ax[0].set_ylabel(&#39;log(Sum-squared-error)&#39;)ax[0].set_title(&#39;Adaline - Learning rate 0.01&#39;)ada2 = AdalineGD(n_iter=10, eta=0.0001).fit(X, y)ax[1].plot(range(1, len(ada2.cost_) + 1),ada2.cost_, marker=&#39;o&#39;)ax[1].set_xlabel(&#39;Epochs&#39;)ax[1].set_ylabel(&#39;Sum-squared-error&#39;)ax[1].set_title(&#39;Adaline - Learning rate 0.0001&#39;)plt.show()</code></pre><p><img src="http://i1.fuimg.com/700703/e8188b7c3918646c.png" alt></p><p>为了优化算法性能，可以对数据做特征缩放，采用以下规则对数据进行特征缩放：<br>$$<br>x_{j}:=\frac{x_j-u_j}{\delta_j}<br>$$<br>$x_j$为训练样本n中第j个特征的所有值的向量，$u_j$、$\delta_j$分别是样本中第j个特征的平均值和标准差。</p><pre><code class="python">X_std = np.copy(X)X_std[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()X_std[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()ada = AdalineGD(n_iter=15, eta=0.01)ada.fit(X_std, y)plot_decision_regions(X_std, y, classifier=ada)plt.title(&#39;Adaline - Gradient Descent&#39;) plt.xlabel(&#39;sepal length [standardized]&#39;)plt.ylabel(&#39;petal length [standardized]&#39;)plt.legend(loc=&#39;upper left&#39;)plt.tight_layout()plt.show()plt.plot(range(1, len(ada.cost_) + 1), ada.cost_, marker=&#39;o&#39;)plt.xlabel(&#39;Epochs&#39;)plt.ylabel(&#39;Sum-squared-error&#39;)plt.show()</code></pre><p><img src="http://i1.fuimg.com/700703/cf61ff26b7cd2770.png" alt></p><p><img src="http://i1.fuimg.com/700703/ae4e566acf916b3d.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>python机器学习分类算法--感知器</title>
      <link href="/2019/10/03/ganzhiji/"/>
      <url>/2019/10/03/ganzhiji/</url>
      
        <content type="html"><![CDATA[<h2 id="感知器基本原理"><a href="#感知器基本原理" class="headerlink" title="感知器基本原理"></a>感知器基本原理</h2><p>定义输入向量x，权值向量w，z是x与w的线性组合，z称作为净输入。</p><p>$$<br>x=\begin{bmatrix}x_{0}<br>\\ x_{1}<br>\\ …<br>\\ x_{m}<br>\end{bmatrix},x_{0}=1,w = \begin{bmatrix}w_{0}<br>\\ w_{1}<br>\\ …<br>\\ w_{m}<br>\end{bmatrix},z = x_{0}w_{0} + x_{1}w_{1} + … + x_{m}w_{m} = x^{T}w,<br>$$</p><p>定义激励函数h(z)，激励函数是一个分段函数，简单来说，当净输入z大于0时，将其划分到1类，否则为 -1类。</p><p>$$<br>h(z) = \begin{cases}<br>1 &amp; \text{ if } z\geq 0 \\<br>-1 &amp; \text{ if } z= else<br>\end{cases}<br>$$</p><p>感知器将输入值乘以权值得到净输入，通过激励函数将样本分为正负两类，感知器的工作过程如下：</p><ol><li>将权重初始化为零或一个极小的随机数。</li><li>迭代所有的训练样本x(i),执行以下操作：</li></ol><p>​    (1)计算输出值$\hat{y} $</p><p>​    (2)更新权重$w_{j} $<br>$$<br>w_{j}:=w_{j}+\Delta w_{j}\\ \Delta w_{j}=\eta(y^{(i)}-\hat{y}^{(i)})x_{j}^{(i)}<br>$$<br>$\eta$为学习率，0~1之间的数。</p><p>通过给每一个特征$x_i$分配对应权重$w_i$，将对应特征和权重相乘后求和得到$z$，然后根据激励函数将z划分为两类，若通过激励函数后所得的输出值（即预测值）与训练数据对应$y_i$相符合，则不改变权重w，若不相符，则改变权重w，使得重新分配的权重在下次分类更接近或者达到真实值。</p><p>需要注意的是，若样本线性不可分，权重w会不断更新，在实际代码实现的时候需要设定一个最大迭代次数。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>定义类Perceptron，创建对象时可选参数有eta即学习率$\eta$，n_iter为迭代次数，可调用fit方法训练模型，调用predict方法测试模型。</p><pre><code>import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom matplotlib.colors import ListedColormapclass Perceptron(object):    def __init__(self, eta=0.01, n_iter=50, random_state=1):                self.eta = eta                self.n_iter = n_iter                self.random_state = random_state    def fit(self, X, y):        rgen = np.random.RandomState(self.random_state)                self.w_ = rgen.normal(loc=0.0, scale=0.01,size=1 + X.shape[1])                self.errors_ = []        for _ in range(self.n_iter):                        errors = 0                        for xi, target in zip(X, y):                                update = self.eta * (target - self.predict(xi))                                self.w_[1:] += update * xi                                self.w_[0] += update                                errors += int(update != 0.0)                        self.errors_.append(errors)                return self    def net_input(self, X):                &quot;&quot;&quot;Calculate net input&quot;&quot;&quot;                return np.dot(X, self.w_[1:]) + self.w_[0]    def predict(self, X):                &quot;&quot;&quot;Return class label after unit step&quot;&quot;&quot;                return np.where(self.net_input(X) &gt;= 0.0, 1, -1)</code></pre><p>从pd库中调用数据集，下表为数据集的最后5条数据。</p><pre><code class="python">df = pd.read_csv(&#39;https://archive.ics.uci.edu/ml/&#39; ...              &#39;machine-learning-databases/iris/iris.data&#39;, ...               header=None) df.tail()</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th {    vertical-align: middle;}.dataframe thead th {    text-align: right;}</code></pre><p></style><p></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>0</th>      <th>1</th>      <th>2</th>      <th>3</th>      <th>4</th>    </tr>  </thead>  <tbody>    <tr>      <th>145</th>      <td>6.7</td>      <td>3.0</td>      <td>5.2</td>      <td>2.3</td>      <td>Iris-virginica</td>    </tr>    <tr>      <th>146</th>      <td>6.3</td>      <td>2.5</td>      <td>5.0</td>      <td>1.9</td>      <td>Iris-virginica</td>    </tr>    <tr>      <th>147</th>      <td>6.5</td>      <td>3.0</td>      <td>5.2</td>      <td>2.0</td>      <td>Iris-virginica</td>    </tr>    <tr>      <th>148</th>      <td>6.2</td>      <td>3.4</td>      <td>5.4</td>      <td>2.3</td>      <td>Iris-virginica</td>    </tr>    <tr>      <th>149</th>      <td>5.9</td>      <td>3.0</td>      <td>5.1</td>      <td>1.8</td>      <td>Iris-virginica</td>    </tr>  </tbody></table></div><p>使用matplotlib库将数据可视化。</p><pre><code class="python">import matplotlib.pyplot as plt# select setosa and versicolor y = df.iloc[0:100, 4].values y = np.where(y == &#39;Iris-setosa&#39;, -1, 1)# extract sepal length and petal length X = df.iloc[0:100, [0, 2]].values# plot data plt.scatter(X[:50, 0], X[:50, 1], label=&#39;setosa&#39;) plt.scatter(X[50:100, 0], X[50:100, 1],marker=&#39;x&#39;,label=&#39;versicolor&#39;)  plt.xlabel(&#39;sepal length [cm]&#39;) plt.ylabel(&#39;petal length [cm]&#39;)plt.legend(loc=&#39;upper left&#39;) plt.show() </code></pre><p><img src="http://i2.tiimg.com/700703/d147555495de8c6b.png" alt="散点图"></p><p>迭代次数和预测错误次数的折线图。</p><pre><code class="python">ppn = Perceptron(eta=0.1,n_iter=10)ppn.fit(X, y)plt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker=&#39;o&#39;) plt.xlabel(&#39;Epochs&#39;)plt.ylabel(&#39;Number of updates&#39;)plt.show()</code></pre><p><img src="http://i2.tiimg.com/700703/0f0a93745808decc.png" alt="折线图"></p><pre><code class="python">def plot_decision_regions(X, y, classifier, resolution=0.02):    # setup marker generator and color map        markers = (&#39;s&#39;, &#39;x&#39;, &#39;o&#39;, &#39;^&#39;, &#39;v&#39;)        colors = (&#39;red&#39;, &#39;blue&#39;, &#39;lightgreen&#39;, &#39;gray&#39;, &#39;cyan&#39;)        cmap = ListedColormap(colors[:len(np.unique(y))])    # plot the decision surface        x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1        x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1        xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),                                                      np.arange(x2_min, x2_max, resolution))        Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)        Z = Z.reshape(xx1.shape)        plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)        plt.xlim(xx1.min(), xx1.max())        plt.ylim(xx2.min(), xx2.max())    # plot class samples        for idx, cl in enumerate(np.unique(y)):                plt.scatter(x=X[y == cl, 0],                                        y=X[y == cl, 1],                                        alpha=0.8,                                        c=colors[idx],                                        marker=markers[idx],                                        label=cl,                                        edgecolor=&#39;black&#39;)plot_decision_regions(X, y, classifier=ppn)plt.xlabel(&#39;sepal length [cm]&#39;)plt.ylabel(&#39;petal length [cm]&#39;)plt.legend(loc=&#39;upper left&#39;)plt.show()</code></pre><p><img src="http://i2.tiimg.com/700703/15a977e327e47b82.png" alt="分类图"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>numpy_矩阵切片</title>
      <link href="/2019/10/03/numpy-array/"/>
      <url>/2019/10/03/numpy-array/</url>
      
        <content type="html"><![CDATA[<p>和python列表一样，numpy库中array也有着可切片的特性，这大大方便了我们对矩阵的可操作性。</p><pre><code class="python">import numpy as npa = np.arange(12)a.shape = (3,4)   #同写法a = a.reshape(3,4)print(a)</code></pre><p>首先我们现创建一个3*4的矩阵，如下：</p><pre><code>[[ 0  1  2  3] [ 4  5  6  7] [ 8  9 10 11]]</code></pre><p>可通过a[x,y]选择第x+1行第y+1列的元素</p><pre><code class="python">print(a[2,3]) #选择矩阵第3行第4列</code></pre><pre><code>11</code></pre><p>通过以下方法可对矩阵进行切割。</p><p>a[:,y]中“：”表示选择所有行，y表示选择第y+1列，a[:,1]表示选择矩阵a第2列的所有行，也就是[1 5 9]；</p><p>a[1,:]表示选择矩阵a第2行的所有列，也就是[4 5 6 7]。</p><p>a[:,:2]表示选择矩阵a第1和第2列的所有行。</p><pre><code class="python">print(a[:,1])  #选择矩阵第二列 print(a[1,:])  #选择矩阵第二行  print(a[:,:2]) #选择矩阵第一和第二列</code></pre><pre><code>[1 5 9][4 5 6 7][[0 1] [4 5] [8 9]]</code></pre><p>选择矩阵第2，3行和第2，3列</p><pre><code>print(a[1:,1:3]) </code></pre><pre><code>[[ 5  6] [ 9 10]]</code></pre><pre><code>print(a[:,[0,2]) #选择矩阵a的第1和第3列[[0 2] [4 6] [8 10]]</code></pre><p>用ravel()方法可将矩阵按行展开</p><pre><code class="python">print(a.ravel())#将矩阵转换成列表</code></pre><pre><code>[ 0  1  2  3  4  5  6  7  8  9 10 11]</code></pre><p>方法np.where(condition,x1,x2)，矩阵元素满足condition则选择用x1替换，否则用x2替换</p><pre><code class="python">y = np.array([&#39;one&#39;,&#39;zreo&#39;,&#39;one&#39;])y = np.where(y == &#39;one&#39;,1,0) #满足条件选1，不满足选0print(y)</code></pre><pre><code>[1 0 1]</code></pre>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>机器学习概述</title>
      <link href="/2019/10/02/intro-machinelearning/"/>
      <url>/2019/10/02/intro-machinelearning/</url>
      
        <content type="html"><![CDATA[<h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><p>近年来，NumPy、matplotlib和pandas等多种功能强大的开源库的出现，以及利用深度学习框架tensorflow等，使得机器学习这一领域绽放了无限的生机。</p><h3 id="机器学习的三种方法"><a href="#机器学习的三种方法" class="headerlink" title="机器学习的三种方法"></a>机器学习的三种方法</h3><ol><li><p>监督学习(supervised learning)</p><p>监督学习处理的问题分为两种，一是分类问题（classification），根据已经标记有是否为垃圾邮件的样本库，判断一封新邮件是否为垃圾邮件的问题。二是回归问题（regression），如我们想预测学生在SAT考试中数学科目的成绩，根据往期学生成绩与学习时间建立训练模型，通过学习时间预测学生的数学成绩。</p><p>监督学习是根据已有的样本训练模型，然后对未知的样本进行预测的一种方法。</p></li><li><p>非监督学习(unsupervised learning)</p><p>无监督学习是在没有已知输出变量和反馈函数指导的情况下提取有效信息来探索数据的整体结构的。通过非监督学习能够发现数据本身潜在的结构，在数据压缩中的降维邻域非监督学习也发挥着重要作用。</p></li><li><p>强化学习(reinforcement learning)</p><p>强化学习构建一个系统，在与环境的交互的过程中提高系统的性能。一个强化学习的经典例子就是象棋对弈游戏，系统根据当前局态（环境）决定落子的位置，游戏结束时胜负的判定可以作为反馈，通过这个反馈让系统做出调整，优化系统，提升系统的性能。</p></li></ol><h3 id="选择不同的算法训练模型"><a href="#选择不同的算法训练模型" class="headerlink" title="选择不同的算法训练模型"></a>选择不同的算法训练模型</h3><p>任何分类算法都有内在的局限性，在实际解决问题的过程中， 应该选用几种不同的算法来训练模型，并比较它们的性能，从中选择最优的一个。一个常用的性能指标是分类的准确性。</p><h3 id="机器学习技能树"><a href="#机器学习技能树" class="headerlink" title="机器学习技能树"></a>机器学习技能树</h3><p><img src="http://i1.fuimg.com/700703/cf43e5cf096624cd.png" alt="机器学习技能图谱"></p><p>图谱从<a href="https://github.com/TeamStuQ/skill-map转载" target="_blank" rel="noopener">https://github.com/TeamStuQ/skill-map转载</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>写在博客之前</title>
      <link href="/2019/10/02/whyblog/"/>
      <url>/2019/10/02/whyblog/</url>
      
        <content type="html"><![CDATA[<h2 id="Why-Blog？"><a href="#Why-Blog？" class="headerlink" title="Why Blog？"></a>Why Blog？</h2><blockquote><p>记录日常学习笔记。</p><p>方便快速复习已经学过的知识。</p><p>增加写文能力。</p><p>希望能帮助到其他人。</p></blockquote><h2 id="工匠精神"><a href="#工匠精神" class="headerlink" title="工匠精神"></a>工匠精神</h2><p>当今社会，经济和技术高速发展，身处于这样一个时代的我们难免有些浮躁。我所推崇的”工匠精神“，是一种刻骨钻研、追求创新的精神。我所希望的是，任何一种职业，不管在学习还是工作中，都应该像一名工匠一样，尽自己所能把事情做好，做精。</p>]]></content>
      
      
      <categories>
          
          <category> 杂谈 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 其他 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
