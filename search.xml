<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>使用git向开源社区贡献自己的代码</title>
      <link href="/2019/11/25/git-practice/"/>
      <url>/2019/11/25/git-practice/</url>
      
        <content type="html"><![CDATA[<p>一 创建git仓库</p><ol><li><p>创建SSH Key</p><pre><code>$ ssh-keygen -t rsa -C &quot;youremail@example.com&quot;</code></pre><p>在.ssh目录下有id_rsa和id_rsa.pub两个文件。</p></li><li><p>在github上将id_rsa.pub公钥添加到SSH Keys中</p></li><li><p>在本地上创建一个git仓库</p><pre><code>$ git init</code></pre></li><li><p>关联远程库</p><pre><code>$ git remote add origin git@github.com:zhongshanhao/learngit.git</code></pre><p>origin是远程仓库的名字，关联远程库后就可以在本地管理git仓库了，这里的仓库是learngit。</p></li></ol><p>二 向开源社区贡献自己的代码</p><ol><li><p>在github上fork项目源代码到自己的远程仓库</p></li><li><p>在本地clone仓库</p><pre><code>git clone git@github.com:zhongshanhao/tidb.git</code></pre></li><li><p>添加远程仓库分支upstream，该分支是项目源代码所在的地方，不是自己克隆的远程仓库</p><pre><code>git remote add upstream https://github.com/pingcap/tidb</code></pre><p>拓展</p><pre><code>git remote -v          # 查看远程仓库配置git fetch upstream   # 将远程仓库的所有更新取回本地git log upstream/master    # 查看远程仓库master分支对应的commit记录git merge upstream/master  # 将远程仓库master与本地分支合并git log --pretty=oneline --graph </code></pre></li><li><p>在开发之前先同步远程仓库</p><pre><code>git fetch upstreamgit merge upstream/master#  或者git pull upstream master</code></pre></li><li><p>然后在本地仓库创建分支进行开发</p><pre><code>git checkout -b my_branch</code></pre></li><li><p>完成代码开发后，将分支推送到远程仓库</p><pre><code>git push origin my_branch:my_branch</code></pre></li><li><p>在github上选择my_branch分支，点击new pull request创建新的PR</p></li><li><p>等待pr完成，可以删除该分支</p><pre><code>git branch -d my_branch</code></pre><p>命令拓展</p><ul><li><p>暂存工作现场</p><pre><code>git log upstream/master    # 查看upstream/master分支最新的commit# 当当前工作没有提交到本地仓库时，可以将工作现场存储起来git stashgit stash list                 # 查看暂存区列表git stash pop             # 恢复工作现场，删除暂存区内容git stash apply stash@{0}   # 恢复指定暂存区，且不删除该暂存区内容git stash drop              # 删除暂存区内容</code></pre></li><li><p>merge出现冲突</p><pre><code>git status  #定位冲突文件#修改文件解决冲突git add .git commit -m &quot;solve conflict&quot;</code></pre></li></ul><p>rebase</p><p>变基操作，将my_branch分支变基，即将该分支的分叉出改为最新的master头结点处</p><pre><code>git checkout my_branchgit rebase mastergit push origin my_branch:my_branch</code></pre><ul><li><p>变基出现冲突时</p><pre><code>git rebase master  #出现冲突#解决冲突git add .git rebase --continue #继续变基</code></pre></li><li><p>git push出现冲突时</p><pre><code>git fetch origin   #拉取远程分支git merge origin/my_branch   #合并对应分支，合并冲突在另外解决#若有冲突,定位文件,修改文件,提交修改git status  git add .git commit -m &quot;fix conflict&quot;#最后pushgit push origin my_branch:my_branch</code></pre></li></ul></li></ol><p>图解</p><img src="http://kmknkk.oss-cn-beijing.aliyuncs.com/image/git.jpg" alt><img src="https://img-blog.csdnimg.cn/20190311173112758.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppb2hvX2NoZW4=,size_16,color_FFFFFF,t_70">]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>多线程</title>
      <link href="/2019/11/17/multiThread/"/>
      <url>/2019/11/17/multiThread/</url>
      
        <content type="html"><![CDATA[<h2 id><a href="#" class="headerlink" title></a><!-- 多线程 --></h2><h3 id="进程与线程"><a href="#进程与线程" class="headerlink" title="进程与线程"></a>进程与线程</h3><h4 id="进程"><a href="#进程" class="headerlink" title="进程"></a>进程</h4><p>是什么？</p><p>在linux是用命令<code>ps aux</code>可以查看正在运行的进程</p><p><img src="https://i.postimg.cc/W1pwk5Y1/linux.png" alt="linux下的进程"></p><p>进程是程序的一次执行过程。在Java中，每次程序运行至少启动两个线程，一个main线程，一个是垃圾收集进程。在比如打开一个浏览器时，系统会创建一个进程，打开一个新的标签页相当于创建了一个线程，一个进程可以拥有多个线程。</p><h4 id="线程"><a href="#线程" class="headerlink" title="线程"></a>线程</h4><p>是什么？</p><p>线程是比进程更小的执行单位，一个进程包含多个线程，线程处理细小的任务，线程被称为是轻量级的线程。</p><h4 id="线程生命周期"><a href="#线程生命周期" class="headerlink" title="线程生命周期"></a>线程生命周期</h4><p><img src="https://i.postimg.cc/Z5fFF6XF/image.png" alt="线程生命周期"></p><h5 id="新建"><a href="#新建" class="headerlink" title="新建"></a>新建</h5><p>new关键字创建了一个线程之后，线程就处于新建状态，</p><p>jvm为线程分配内存，初始化成员变量值。</p><h5 id="就绪"><a href="#就绪" class="headerlink" title="就绪"></a>就绪</h5><p> 当线程对象调用了start() 方法之后，该线程就处于就绪状态，就绪态的线程只缺少处理器资源就可以运行了；</p><p>jvm为线程创建方法栈和程序技术器，等待线程调度器调度。</p><h5 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h5><p>就绪态的线程获得cpu资源，开始运行run()，此时线程进入运行状态。</p><h5 id="阻塞"><a href="#阻塞" class="headerlink" title="阻塞"></a>阻塞</h5><p>线程可以主动或被动放弃cpu资源，进入阻塞状态</p><ul><li>线程调用sleep()主动放弃cpu</li><li>线程等待io</li><li>线程尝试获得一个同步锁，但是该同步锁被其他线程所占用</li><li>线程在等待某个通知(notify)</li><li>程序调用了线程的suspend()方法将线程挂起。该方法容易导致死锁。</li></ul><h5 id="死亡"><a href="#死亡" class="headerlink" title="死亡"></a>死亡</h5><p>线程可能有以下3种方式死亡</p><ul><li>run()或call()方法执行完成，线程正常结束。</li><li>线程执行出错，抛出异常</li><li>调用stop()方法结束线程，该方法容易产生死锁。</li></ul><h4 id="线程安全"><a href="#线程安全" class="headerlink" title="线程安全"></a>线程安全</h4><p>若一个程序使用了多个线程，它每次的运行结果和只使用单线程的结果是一样的，那么就称为线程安全的。若一个程序中对同一个变量有读和写两个线程，因为我们不知道这两个线程的执行顺序，先执行读和先执行写的两个进程读出来的结果是不一样的，这就是线程不安全。</p><p>例子：多个窗口进行售票，在程序中一个窗口绑定一个线程运行，若没有适当的线程同步措施，可能会造成一张票被售出多次的情况。</p><h4 id="线程的同步方法"><a href="#线程的同步方法" class="headerlink" title="线程的同步方法"></a>线程的同步方法</h4><ul><li>同步代码块（synchronized）</li><li>同步方法（synchronized）</li><li>同步锁（ReenreantLock）</li><li>特殊域变量（volatile）</li><li>局部变量（ThreadLocal）</li><li>阻塞队列（LinkedBlockingQueue）</li><li>原子变量（Atomic*）</li></ul><p>同步代码块（synchronized）</p><p>synchronized关键字可以用于方法中的某个区块中，表示只对该区块的资源进行互斥访问。</p><pre><code>synchronized(同步锁){    需要同步的代码}</code></pre><p>同步锁</p><p>多个线程使用同一把锁，在一个线程拥有同步锁的时候，该线程就可以进入代码块，其他线程只能在外面等着。</p><p>同步方法（synchronized）</p><pre><code>public synchronized void methon(){     需要同步的代码}</code></pre><p>同步锁（ReenreantLock）</p><pre><code>public void lock();            // 开启同步锁public void unlock();   // 释放同步锁try{    lock();    ....}finally{    unlock();}</code></pre><h3 id="死锁"><a href="#死锁" class="headerlink" title="死锁"></a>死锁</h3><p>死锁是多个线程互相抢占资源的造成的一种僵局，导致线程之间相互等待，若没有外力作用，线程将永远等待下去。</p><h4 id="死锁产生的必要条件"><a href="#死锁产生的必要条件" class="headerlink" title="死锁产生的必要条件"></a>死锁产生的必要条件</h4><p>若产生死锁，则下面条件必然成立。</p><ul><li>互斥条件，即进程对所分配的进程</li><li>不可剥夺条件</li><li>占有并请求条件</li><li>循环等待条件</li></ul><p>####　死锁预防</p><p>防止死锁产生的四个必要条件成立即可。</p><h4 id="死锁避免"><a href="#死锁避免" class="headerlink" title="死锁避免"></a>死锁避免</h4><ul><li>有序资源分配法</li><li>银行家算法</li><li>顺序加锁</li><li>限时加锁</li></ul><h4 id="死锁检测"><a href="#死锁检测" class="headerlink" title="死锁检测"></a>死锁检测</h4><p>预防和避免死锁系统开销大且不能充分利用资源，更好的方法是不采取任何限制性措施，而是提供检测和解脱死锁的手段，这就是死锁检测和恢复。</p><p>死锁检测数据结构：</p><ul><li>E是现有资源向量（existing resource vector），代码每种已存在资源的总数</li><li>A是可用资源向量（available resource vector），那么Ai表示当前可供使用的资源数（即没有被分配的资源）</li><li>C是当前分配矩阵（current allocation matrix），C的第i行代表Pi当前所持有的每一种类型资源的资源数</li><li>R是请求矩阵（request matrix），R的每一行代表P所需要的资源的数量</li></ul><img src="https://i.postimg.cc/DfbWjsJP/image.png"> <p>死锁检测步骤</p><ul><li><p>寻找一个没有结束标记的进程Pi，对于它而言R矩阵的第i行向量小于或等于A。</p></li><li><p>如果找到了这样一个进程，执行该进程，然后将C矩阵的第i行向量加到A中，标记该进程，并转到第1步</p></li><li><p>如果没有这样的进程，那么算法终止</p></li><li><p>算法结束时，所有没有标记过的进程都是死锁进程。</p></li></ul><h4 id="死锁恢复"><a href="#死锁恢复" class="headerlink" title="死锁恢复"></a>死锁恢复</h4><ul><li>利用抢占资源恢复，即临时将某个资源从它的当前所属进程转移到另一个进程，相当于破坏了死锁产生的必要条件的不可剥夺条件。</li><li>利用回滚恢复，周期性的将进程的状态进行备份，当发现进程死锁后，根据备份将进程复位到一个更早的，还没有取得所需资源的状态，接着就把这些资源分配给其他进程。</li><li>通过杀死进程恢复，直接将一个或多个进程杀死，相当于破坏了循环等待条件。</li></ul><h3 id="多线程特性"><a href="#多线程特性" class="headerlink" title="多线程特性"></a>多线程特性</h3><p>当线程参与计算时，原始的数据来自内存，在计算过程中，有些数据可能被频繁读取，这些数据被临时存储在寄存器和高速缓存中，当线程完成计算任务后，这些缓存的数据会写回内存。</p><p>当多个线程同时读写某个内存数据时，就会产生多线程并发问题，解决这些问题涉及到多线程编程的三个特性：原子性，有序性，可见性。</p><p>多线程编程应该满足三个特性：原子性，可见性，有序性。</p><p>原子性</p><p>一个或多个操作要么全部执行成功要么就都不执行。</p><p>可见性</p><p>当多个线程访问同一变量时，一个线程修改了这个变量的值，其他线程应该能够立即看到修改的值。</p><p>有序性</p><p>程序的执行顺序按照代码的先后顺序执行。</p><h3 id="多线程控制类"><a href="#多线程控制类" class="headerlink" title="多线程控制类"></a>多线程控制类</h3><p>为了保证多线程的三个特性，Ｊａｖａ引入了很多线程控制机制：</p><ul><li>ThreadLocal</li><li>原子类</li><li>Lock类</li><li>Volatile关键字</li></ul><h4 id="ThreadLocal"><a href="#ThreadLocal" class="headerlink" title="ThreadLocal"></a>ThreadLocal</h4><p>未完待续～</p>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Java内存模型</title>
      <link href="/2019/11/15/java_internal_memory_model/"/>
      <url>/2019/11/15/java_internal_memory_model/</url>
      
        <content type="html"><![CDATA[<h2 id><a href="#" class="headerlink" title></a><!-- Java内存模型 --></h2><h3 id="程序执行流程"><a href="#程序执行流程" class="headerlink" title="程序执行流程"></a>程序执行流程</h3><p><img src="https://i.postimg.cc/CKzCvzgy/java.png" alt></p><ul><li>首先Java源代码文件(.java后缀)会被Java编译器编译为字节码文件(.class后缀)；</li><li>然后由JVM中的类加载器加载各个类的字节码文件到运行时数据区；</li><li>加载完毕之后，交由JVM执行引擎执行。</li></ul><p>Java内存模型指的就是Runtime Data Area（运行时数据区），即程序执行期间用到的数据和相关信息保存区。</p><h3 id="内存模型"><a href="#内存模型" class="headerlink" title="内存模型"></a>内存模型</h3><p><img src="https://i.postimg.cc/YSZ6TMrv/java.png" alt="java运行时数据区"></p><p> JVM 内存共分为虚拟机栈、堆、方法区、程序计数器、本地方法栈五个部分。</p><h4 id="方法区（Java堆的永久区）"><a href="#方法区（Java堆的永久区）" class="headerlink" title="方法区（Java堆的永久区）"></a>方法区（Java堆的永久区）</h4><ul><li>方法区存放了要加载的类的信息（名称、修饰符等）、类中的静态常量等</li><li>方法区被Java线程共享的</li><li>方法区要使用的内存超过其允许的大小时，会抛出OutOfMemoryError: PremGen space的错误信息。</li></ul><h4 id="常量池ConstantPool"><a href="#常量池ConstantPool" class="headerlink" title="常量池ConstantPool"></a>常量池ConstantPool</h4><ul><li><p>常量池是方法区的一部分。</p></li><li><p>常量池中存储两类数据：字面量和引用量。</p><p>字面量：字符串、final变量等。</p><p>引用量：类/接口、方法和字段的名称和描述符，</p></li><li><p>常量池在编译期间就被确定，并保存在已编译的.class文件中 </p></li></ul><p>####　本地方法栈</p><ul><li>本地方法栈和Java栈所发挥的作用非常相似，区别不过是Java栈为JVM执行Java方法服务，而本地方法栈为JVM执行Native方法服务。</li><li>本地方法栈也会抛出StackOverflowError和OutOfMemoryError异常。</li></ul><h4 id="程序计数器"><a href="#程序计数器" class="headerlink" title="程序计数器"></a>程序计数器</h4><ul><li>JVM会为每个线程分配一个程序计数器，各个线程之间计数器相互独立。</li><li>程序计数器记录线程正在执行的内存地址，以便中断恢复后继续从中断位置执行。</li></ul><h4 id="Java虚拟机栈"><a href="#Java虚拟机栈" class="headerlink" title="Java虚拟机栈"></a>Java虚拟机栈</h4><ul><li>每个线程对应一个Java栈，每个Java栈由若干个栈帧组成，每个方法对应一个栈帧</li><li>栈顶的栈帧叫活动栈，表示当前执行的方法，它被cpu执行，方法执行完时，该栈帧弹出栈帧中的元素作为该方法的返回值</li><li>当线程请求的栈深度大于虚拟机所允许的深度，将会抛出StackOverflowError异常</li><li>栈扩展时无法申请到足够的内存，就会抛出OutOfMemoryErroe异常</li></ul><h3 id="Java内存模型工作示意图"><a href="#Java内存模型工作示意图" class="headerlink" title="Java内存模型工作示意图"></a>Java内存模型工作示意图</h3><p><img src="https://i.postimg.cc/zXFkvsjQ/Java.png" alt="Java内存模型工作示意图"></p><h4 id="程序执行流程-1"><a href="#程序执行流程-1" class="headerlink" title="程序执行流程"></a>程序执行流程</h4><ol><li>首先类加载器将Java代码加载到方法区</li><li>然后执行引擎从方法区找到main方法</li><li>为方法创建栈帧放入方法栈，同时创建该栈帧的程序计数器</li><li>执行引擎请求CPU执行该方法</li><li>CPU将方法栈数据加载到工作内存（寄存器和高速缓存），执行该方法</li><li>CPU执行完之后将执行结果从工作内存同步到主内存</li></ol>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>java基础</title>
      <link href="/2019/11/08/java%E5%9F%BA%E7%A1%80/"/>
      <url>/2019/11/08/java%E5%9F%BA%E7%A1%80/</url>
      
        <content type="html"><![CDATA[<h2 id><a href="#" class="headerlink" title></a><!--Java 基础--></h2><h3 id="面向对象三大特性"><a href="#面向对象三大特性" class="headerlink" title="面向对象三大特性"></a>面向对象三大特性</h3><ol><li><p>封装</p><ul><li><p>是什么？</p><p>在Java中，一切皆对象。封装，就是把客观事物封装成抽象的类，一个类就是一个封装了数据以及操作这些数据的代码的逻辑实体。</p></li><li><p>有什么好处？</p><p>我们将复杂的功能封装到一个类中，对外开放一个接口，我们在使用的时候不用管里面复杂的逻辑，直接调用即可完成功能。</p></li></ul></li><li><p>继承</p><ul><li><p>是什么？<br>继承是一个对象获取另一个一个对象中属性的方法。</p></li><li><p>有什么好处？<br>继承可以大幅减少冗余的代码，并且可基于父类的基础上扩展代码，增加功能，提高开发效率。</p></li></ul></li><li><p>多态</p><ul><li><p>是什么？<br>用超类可引用其所有的子类，若不同的子类覆写了父类的一个方法，则用超类引用不同子类时调用该方法会执行实际子类的方法，这种由一个对象引用时调用方法反映的是实际子类的方法的特性叫做多态。</p></li><li><p>有什么好处？</p><p>提高了代码的扩展性。</p></li></ul></li></ol><h3 id="String、StringBuffer和StringBuilder的区别是什么？"><a href="#String、StringBuffer和StringBuilder的区别是什么？" class="headerlink" title="String、StringBuffer和StringBuilder的区别是什么？"></a>String、StringBuffer和StringBuilder的区别是什么？</h3><ol><li><p>相同点</p><p>都是用char数组保存数据的。</p></li><li><p>区别</p><p>String类中用来保存字符数据的数组的修饰符为final，这表明String对象一旦创建，其中的保存的字符数据就是不变的，也可以说String类是不可变的，所以要保存另一串字符数据需要创建一个新的String对象，而StringBuffer和StringBuilder中保存的字符数组是可变的，所以要保存另一串字符数据时无须新建一个对象，直接修改即可。</p><p>StringBuffer对方法加了同步锁，String中的字符数据是不可变的，所以这两者拥有线程安全性，StringBuilder则没有线程安全。由于没有线程安全和不用每次保存数据的时候创建对象，所以在操作多字符串数据时，StringBuilder相对来说运行速度更快，StringBuffer则相对慢些，String最慢。</p></li></ol><h3 id="修饰符"><a href="#修饰符" class="headerlink" title="修饰符"></a>修饰符</h3><ol><li><code>final</code></li></ol><ul><li><p>作用在类上<br>表示该类不可以被继承</p></li><li><p>作用在方法上<br>表示该方法不可以被覆写</p><ul><li>作用在字段上<br>表示该字段不能被修改</li></ul></li></ul><ol start="2"><li><p><code>static</code></p><ul><li><p>作用在成员变量上<br>被称为静态字段，静态字段只有一个共享的空间，在类的所有实例中共享静态字段。</p></li><li><p>作用在方法上<br>被称为静态方法，静态方法能够在不创建实例的情况下通过类名直接调用，如<code>Math.max()</code>；<br>静态方法不能调用非静态的方法和成员变量。</p></li></ul></li><li><p><code>private</code></p><p>私有访问修饰符，被声明为private的方法、变量和构造方法只能被所属类访问，注意类和接口不能声明为private。</p></li><li><p><code>public</code></p><p>与private相反，被声明为public的方法、变量、类和接口可以被任何其他类访问。</p></li></ol><h3 id="与-equals"><a href="#与-equals" class="headerlink" title="== 与 equals"></a>== 与 equals</h3><ol><li><p>==<br>它的作用是判断两个对象的地址相不相等。即判断两个对象是不是同一个对象，基本数据类型中”==”比较的是值，而引用数据类型中”==”比较的是内存地址。（基本数据类型：如int,double,float,char等，引用数据类型：如String，StringBuilder等）</p></li><li><p>equals</p><p>可以重写该方法，equals一般用来比较两个对象的值是否相等，在String类中使用较为频繁，判断两个字符串是否相等可以用一下办法：</p></li></ol><pre><code>            if(str1 != null &amp;&amp; str1.equals(str2)){                ...            }</code></pre><h3 id="hashCode-与-equals"><a href="#hashCode-与-equals" class="headerlink" title="hashCode 与 equals"></a>hashCode 与 equals</h3><p>若要将类存入散列表中，重写equals方法时必须要重写hashCode。</p><ol><li><p>hashCode</p><ul><li><p>是什么？<br>在你要将对象存入散列表时，需要使用<code>hashCode()</code>方法得到一个整数，这个整数称为哈希码，标识着对象在散列表中的位置，一般使用对象中的成员变量的值来计算哈希码。</p></li><li><p>有什么用？<br>根据哈希码可以将对象存入到散列表的特定位置，下次需要检索该对象时会直接计算哈希码从而快速定位到对象在散列表中的位置，大大缩短搜索时间。</p></li></ul></li><li><p>为什么要有hashCode<br>当我们使用散列表时，会根据对象的哈希码得出其在散列表中的位置，若散列表对应位置中已经存有数据了，说明两个对象的哈希码相同，这时候equals就登场了，使用equals判断两个对象是否相等，若相等，则在不加入散列表，若不相等，则表明两个对象不相等，这时候就重新将该对象散列到其他位置。</p></li><li><p>所以<br>两个对象相等，则他们的哈希码一定相同；<br>两个对象相等，他们的哈希码不一定相同。</p></li></ol><h3 id="深拷贝-vs-浅拷贝"><a href="#深拷贝-vs-浅拷贝" class="headerlink" title="深拷贝 vs 浅拷贝"></a>深拷贝 vs 浅拷贝</h3><ol><li><p>深拷贝</p><p>对基本数据类型进行值传递，对引用数据类型进行引用传递般的拷贝。</p></li><li><p>浅拷贝<br>对基本数据类型进行值传递，对引用数据类型，创建一个新的对象，并复制其内容。</p></li></ol><h3 id="Java集合"><a href="#Java集合" class="headerlink" title="Java集合"></a>Java集合</h3><h4 id="List是一种有序列表的集合"><a href="#List是一种有序列表的集合" class="headerlink" title="List是一种有序列表的集合"></a>List是一种有序列表的集合</h4><p>有两个实现方式，一是采用ArrayList，在该类内部使用数组存储数据，二是采用LinkedList，内部使用双向链表存储数据。</p><table><thead><tr><th>操作</th><th>ArrayList</th><th>LinkedList</th></tr></thead><tbody><tr><td>获取指定位置元素</td><td>速度快</td><td>需要从头开始查找</td></tr><tr><td>在指定位置添加或者删除</td><td>速度慢，需要移动元素</td><td>速度快，无需移动元素</td></tr><tr><td>内存占用情况</td><td>少</td><td>较大</td></tr></tbody></table><p>对比上表，综合来说优先使用ArrayList。</p><p>最好使用迭代器来遍历List，如果使用普通for循环遍历的方式，当实现类是LinkedList时，每次查找指定位置元素时都需要从头开始查找。</p><h4 id="Map采用键值查找的映射表集合"><a href="#Map采用键值查找的映射表集合" class="headerlink" title="Map采用键值查找的映射表集合"></a>Map采用键值查找的映射表集合</h4><p>使用key-value对存储元素，key不可以重复，value可以重复。</p><p>有两个实现类</p><ul><li>HashMap：非线程安全的（ConcurrentHashMap类是线程安全的）；底层数据结构采用红黑树存储元素，红黑树是一颗自平衡二叉查找树，它可以在时间复杂度O(log n)下完成数据的查找、删除添加。</li><li>Hashtable：线程安全，但是由于效率问题，这个类基本被淘汰了。</li></ul><p>HashMap底层实现</p><p>采用数组加链表方式存储（拉链法），当两个元素的hashCode相同时，在数组相应的位置链上一个链表，来解决冲突。为解决查找效率问题，当链表长度大于8时，将单链表改为红黑树方式存储冲突的元素。</p><h4 id="Set是没有重复元素的集合"><a href="#Set是没有重复元素的集合" class="headerlink" title="Set是没有重复元素的集合"></a>Set是没有重复元素的集合</h4><p>Set存储的元素是不重复的，要判断存入的元素之间是否相等，需要正确覆写equals()和hashCode()方法。equals()用来判断两个元素是否相等，hashCode()方法用来求出元素在散列表的位置。</p><p>Set集合有两个实现类，分别是HashSet和TreeSet：</p><ul><li>HashSet是无序的，HashSet是基于HashMap实现的，HashSet只存储key；</li><li>TreeSet是有序的，因为它实现了SortedSet接口。</li></ul><h4 id="未完待续～"><a href="#未完待续～" class="headerlink" title="未完待续～"></a>未完待续～</h4>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>python机器学习-数据预处理</title>
      <link href="/2019/10/10/data-preprocessing/"/>
      <url>/2019/10/10/data-preprocessing/</url>
      
        <content type="html"><![CDATA[<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><h3 id="处理非数值型数据"><a href="#处理非数值型数据" class="headerlink" title="处理非数值型数据"></a>处理非数值型数据</h3><pre><code class="python">import pandas as pd  df = pd.DataFrame([                     [&#39;green&#39;, &#39;M&#39;, 10.1, &#39;class1&#39;],                     [&#39;red&#39;, &#39;L&#39;, 13.5, &#39;class2&#39;],                     [&#39;blue&#39;, &#39;XL&#39;, 15.3, &#39;class1&#39;]]) df.columns = [&#39;color&#39;, &#39;size&#39;, &#39;price&#39;, &#39;classlabel&#39;] print(df)</code></pre><pre><code>   color size  price classlabel0  green    M   10.1     class11    red    L   13.5     class22   blue   XL   15.3     class1</code></pre><p>利用pandas将CSV格式的数据读取进来，要将字符串转换为适合机器学习算法训练的数据类型。<br>先对特征size数据进行处理，由于XL&gt;L&gt;M，size特征有这样的一个顺序特征，所以将这类字符串转换为数值时也该保留顺序特征。利用映射函数map很容易将字符串转换：</p><pre><code class="python">size_mapping = {                                  &#39;XL&#39;: 3,                                  &#39;L&#39;: 2,                                &#39;M&#39;: 1} df[&#39;size&#39;] = df[&#39;size&#39;].map(size_mapping) print(df)</code></pre><pre><code>   color  size  price classlabel0  green     1   10.1     class11    red     2   13.5     class22   blue     3   15.3     class1</code></pre><p>可以利用小技巧将数值数据转换回字符串：</p><pre><code class="python">inv_size_mapping = {v: k for k, v in size_mapping.items()}print(df[&#39;size&#39;].map(inv_size_mapping) )</code></pre><pre><code>0     M1     L2    XLName: size, dtype: object</code></pre><p>对于分类标签classlabe，只需要将字符串转换为数值特征就好。<br>利用枚举函数enumerate()和np.unique()将标签导出为映射字典，在用映射函数map将分类标签转换为整数：</p><pre><code class="python">import numpy as npclass_mapping = {label:idx for idx,label in                                  enumerate(np.unique(df[&#39;classlabel&#39;]))}df[&#39;classlabel&#39;] = df[&#39;classlabel&#39;].map(class_mapping) print(df)</code></pre><pre><code>   color  size  price  classlabel0  green     1   10.1           01    red     2   13.5           12   blue     3   15.3           0</code></pre><p>同样将整数值可以反映射回分类标签：</p><pre><code class="python">inv_class_mapping = {v: k for k, v in class_mapping.items()}df[&#39;classlabel&#39;] = df[&#39;classlabel&#39;].map(inv_class_mapping) print(df)</code></pre><pre><code>   color  size  price classlabel0  green     1   10.1     class11    red     2   13.5     class22   blue     3   15.3     class1</code></pre><p>也可直接调用sklearn库LabelEncoder类实现：</p><pre><code class="python">from sklearn.preprocessing import LabelEncoder class_le = LabelEncoder() y = class_le.fit_transform(df[&#39;classlabel&#39;].values) print(y)</code></pre><pre><code>[0 1 0]</code></pre><pre><code class="python">class_le.inverse_transform(y)</code></pre><pre><code>array([&#39;class1&#39;, &#39;class2&#39;, &#39;class1&#39;], dtype=object)</code></pre><p>接下来考虑特征color，颜色green，red，blue没有顺序特征，可以采用热编码的方式将其转换，调用pandas中的get_dummies方法转换字符串：</p><pre><code class="python">pd.get_dummies(df[[&#39;price&#39;, &#39;color&#39;, &#39;size&#39;]])</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th {    vertical-align: top;}.dataframe thead th {    text-align: right;}</code></pre><p></style><p></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>price</th>      <th>size</th>      <th>color_blue</th>      <th>color_green</th>      <th>color_red</th>    </tr>  </thead>  <tbody>    <tr>      <td>0</td>      <td>10.1</td>      <td>1</td>      <td>0</td>      <td>1</td>      <td>0</td>    </tr>    <tr>      <td>1</td>      <td>13.5</td>      <td>2</td>      <td>0</td>      <td>0</td>      <td>1</td>    </tr>    <tr>      <td>2</td>      <td>15.3</td>      <td>3</td>      <td>1</td>      <td>0</td>      <td>0</td>    </tr>  </tbody></table></div><h3 id="划分数据集为训练集和测试集"><a href="#划分数据集为训练集和测试集" class="headerlink" title="划分数据集为训练集和测试集"></a>划分数据集为训练集和测试集</h3><pre><code class="python">import pandas as pdimport numpy as npdf_wine = pd.read_csv(&#39;https://archive.ics.uci.edu/&#39;                                                 &#39;ml/machine-learning-databases/&#39;                                                 &#39;wine/wine.data&#39;, header=None) df_wine.columns = [&#39;Class label&#39;, &#39;Alcohol&#39;,                                         &#39;Malic acid&#39;, &#39;Ash&#39;,                                       &#39;Alcalinity of ash&#39;, &#39;Magnesium&#39;,                                        &#39;Total phenols&#39;, &#39;Flavanoids&#39;,                                           &#39;Nonflavanoid phenols&#39;,                                           &#39;Proanthocyanins&#39;,                                            &#39;Color intensity&#39;, &#39;Hue&#39;,                                            &#39;OD280/OD315 of diluted wines&#39;,                                            &#39;Proline&#39;]  print(&#39;Class labels&#39;, np.unique(df_wine[&#39;Class label&#39;])) df_wine.head()</code></pre><pre><code>Class labels [1 2 3]</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th {    vertical-align: top;}.dataframe thead th {    text-align: right;}</code></pre><p></style><p></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>Class label</th>      <th>Alcohol</th>      <th>Malic acid</th>      <th>Ash</th>      <th>Alcalinity of ash</th>      <th>Magnesium</th>      <th>Total phenols</th>      <th>Flavanoids</th>      <th>Nonflavanoid phenols</th>      <th>Proanthocyanins</th>      <th>Color intensity</th>      <th>Hue</th>      <th>OD280/OD315 of diluted wines</th>      <th>Proline</th>    </tr>  </thead>  <tbody>    <tr>      <td>0</td>      <td>1</td>      <td>14.23</td>      <td>1.71</td>      <td>2.43</td>      <td>15.6</td>      <td>127</td>      <td>2.80</td>      <td>3.06</td>      <td>0.28</td>      <td>2.29</td>      <td>5.64</td>      <td>1.04</td>      <td>3.92</td>      <td>1065</td>    </tr>    <tr>      <td>1</td>      <td>1</td>      <td>13.20</td>      <td>1.78</td>      <td>2.14</td>      <td>11.2</td>      <td>100</td>      <td>2.65</td>      <td>2.76</td>      <td>0.26</td>      <td>1.28</td>      <td>4.38</td>      <td>1.05</td>      <td>3.40</td>      <td>1050</td>    </tr>    <tr>      <td>2</td>      <td>1</td>      <td>13.16</td>      <td>2.36</td>      <td>2.67</td>      <td>18.6</td>      <td>101</td>      <td>2.80</td>      <td>3.24</td>      <td>0.30</td>      <td>2.81</td>      <td>5.68</td>      <td>1.03</td>      <td>3.17</td>      <td>1185</td>    </tr>    <tr>      <td>3</td>      <td>1</td>      <td>14.37</td>      <td>1.95</td>      <td>2.50</td>      <td>16.8</td>      <td>113</td>      <td>3.85</td>      <td>3.49</td>      <td>0.24</td>      <td>2.18</td>      <td>7.80</td>      <td>0.86</td>      <td>3.45</td>      <td>1480</td>    </tr>    <tr>      <td>4</td>      <td>1</td>      <td>13.24</td>      <td>2.59</td>      <td>2.87</td>      <td>21.0</td>      <td>118</td>      <td>2.80</td>      <td>2.69</td>      <td>0.39</td>      <td>1.82</td>      <td>4.32</td>      <td>1.04</td>      <td>2.93</td>      <td>735</td>    </tr>  </tbody></table></div><p>首先将数据读取，调用train_test_split函数将数据集划分，参数test_size=0.3表示将数据集的30%<br>划分给测试集，把70%划分给训练集，stratify=y表示训练集中各个类别的比例和测试集中一样。</p><pre><code class="python">from sklearn.model_selection import train_test_split X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values X_train, X_test, y_train, y_test =train_test_split(X, y,                                                                   test_size=0.3,                                                                   random_state=0,                                                                  stratify=y)</code></pre><h3 id="数据正则化"><a href="#数据正则化" class="headerlink" title="数据正则化"></a>数据正则化</h3><p>数据正则化对线性模型和梯度下降等优化算法特别有用。但基于树的模型不需要进行标准化。<br>创建StandardScaler对象，调用fit方法获取样本均值和标准差，再用这些参数去转换测试集。</p><pre><code class="python">from sklearn.preprocessing import StandardScaler sc = StandardScaler() sc.fit(X_train) X_train_std = sc.transform(X_train)X_test_std = sc.transform(X_test) </code></pre><h3 id="特征的选择"><a href="#特征的选择" class="headerlink" title="特征的选择"></a>特征的选择</h3><p>L1正则化：</p><pre><code class="python">import matplotlib.pyplot as pltfrom sklearn.linear_model import LogisticRegressionfig = plt.figure() ax = plt.subplot(111)colors = [&#39;blue&#39;, &#39;green&#39;, &#39;red&#39;, &#39;cyan&#39;,           &#39;magenta&#39;, &#39;yellow&#39;, &#39;black&#39;,           &#39;pink&#39;, &#39;lightgreen&#39;, &#39;lightblue&#39;,           &#39;gray&#39;, &#39;indigo&#39;, &#39;orange&#39;] weights, params = [], []for c in np.arange(-4., 6.):     lr = LogisticRegression(penalty=&#39;l1&#39;,                             C=10.**c,                             random_state=0,solver=&#39;liblinear&#39;,multi_class=&#39;ovr&#39;)     lr.fit(X_train_std, y_train)    weights.append(lr.coef_[1])    params.append(10**c)weights = np.array(weights)for column, color in zip(range(weights.shape[1]), colors):    plt.plot(params, weights[:, column],             label=df_wine.columns[column + 1],             color=color) plt.axhline(0, color=&#39;black&#39;, linestyle=&#39;--&#39;, linewidth=3)plt.xlim([10**(-5), 10**5])plt.ylabel(&#39;weight coefficient&#39;)plt.xlabel(&#39;C&#39;) plt.xscale(&#39;log&#39;)plt.legend(loc=&#39;upper left&#39;)ax.legend(loc=&#39;upper center&#39;,          bbox_to_anchor=(1.38, 1.03),          ncol=1, fancybox=True)plt.show() </code></pre><p><img src="https://i.postimg.cc/BvgfKZyw/4.png" alt></p><p>纵轴对应各个特征的权重，横轴参数C是正则化参数的逆，参数C越小，模型的正则化强度就越强，上述采用L1正则化，可以看到，当C极小的时候，各个特征的权值都趋近于0，然而，当C越大时，有些特征的权重依然徘徊在0附近，这说明对于该分类模型来说，这些特征对于分类的作用没有那么明显，所以，我们可以在训练模型的时候将这些模型去掉，以简化分类模型，减少泛化误差。可以说，L1正则化是一种特征选择技术。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>git常用命令</title>
      <link href="/2019/10/06/git-command/"/>
      <url>/2019/10/06/git-command/</url>
      
        <content type="html"><![CDATA[<h2 id="创建仓库"><a href="#创建仓库" class="headerlink" title="创建仓库"></a>创建仓库</h2><ol><li>mkdir learngit  创建文件夹</li><li>cd learngit      </li><li>git init        将当前所在的目录变成git可以管理的仓库</li></ol><h2 id="将文件加入git管理仓库"><a href="#将文件加入git管理仓库" class="headerlink" title="将文件加入git管理仓库"></a>将文件加入git管理仓库</h2><ol><li>git add readme.txt 将文件存入git暂存区</li><li>git commit -m “wrote a readme file”   将文件提交给git仓库</li></ol><h2 id="工作区"><a href="#工作区" class="headerlink" title="工作区"></a>工作区</h2><ol><li>git status   查看工作区状态</li><li>Git diff readme.txt 查看工作区和git仓库中文件的不同</li></ol><h2 id="版本回退"><a href="#版本回退" class="headerlink" title="版本回退"></a>版本回退</h2><ol><li>git log –pretty=oneline 查看提交日志</li><li>git reset –hard HEAD^ 版本回退到上一个提交版本 HEAD为指向当前版本指针</li><li>git reset –hard 1094a 版本回到特定版本号，可用git log命令查看对应的版本号</li></ol><h2 id="删除文件"><a href="#删除文件" class="headerlink" title="删除文件"></a>删除文件</h2><ol><li>rm readme.txt  在工作区删除文件</li><li>git rm readme.txt 在git仓库删除文件</li><li>git checkout – readme.txt 从git仓库中恢复文件到工作区 前提git仓库有readme.txt</li></ol><h2 id="添加远程库"><a href="#添加远程库" class="headerlink" title="添加远程库"></a>添加远程库</h2><ol><li>git remote add origin <a href="mailto:git@github.com" target="_blank" rel="noopener">git@github.com</a>:zhongshanhao/learngit.git 关联远程库</li><li>git push -u origin master 第一次推送master分支的所有内容</li><li>git push origin master 推送master分支的所有内容</li></ol><h2 id="克隆远程库"><a href="#克隆远程库" class="headerlink" title="克隆远程库"></a>克隆远程库</h2><p>1.git clone <a href="mailto:git@github.com" target="_blank" rel="noopener">git@github.com</a>:zhongshanhao/learngit.git</p><h2 id="查看当前连接的远程库"><a href="#查看当前连接的远程库" class="headerlink" title="查看当前连接的远程库"></a>查看当前连接的远程库</h2><p>1.git remote -v</p><h2 id="本地同步远程仓库"><a href="#本地同步远程仓库" class="headerlink" title="本地同步远程仓库"></a>本地同步远程仓库</h2><p>1.git pull        </p><p>2.git remote set-url origin <a href="https://github.com/zhongshanhao/python.git" target="_blank" rel="noopener">https://github.com/zhongshanhao/python.git</a></p><p>3.git pull origin master –allow-unrelated-histories //把远程仓库和本地同步，消除差异 </p>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>python机器学习分类算法--自适应线性神经网络（Adaline）</title>
      <link href="/2019/10/05/AdalineGd/"/>
      <url>/2019/10/05/AdalineGd/</url>
      
        <content type="html"><![CDATA[<h2 id="算法初步"><a href="#算法初步" class="headerlink" title="算法初步"></a>算法初步</h2><p>Adaline可以看作是对感知器算法的优化和改进，Adaline算法定义了最小化连续性代价函数的概念，这为理解如逻辑回归、支持向量机和回归模型等更高级的机器学习算法奠定了基础。</p><p>定义代价函数：<br>$$<br>J(w) = \frac{1}{2}\sum_{i=1}^{m}(y^{(i)}-h_w(x^{(i)}))^2<br>\\ h_w(x) = w^Tx<br>$$<br>我们要优化目标函数$h_w(x)$，使得输出值符合实际值，就要尽可能降低代价函数$J(w)$,找到使得代价函数最小的w。</p><p>利用梯度下降求最优解：<br>$$<br>w_j:=w_j-\Delta w_j \<br> \Delta w_j = \eta \frac{\delta J}{\delta w_j}=\eta \sum_{i=1}^{m}(h_w(x^{(i)})-y^{(i)})x_j^{(i)}<br>$$</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>定义类AdalineGD封装分类算法，调用fit方法拟合数据，训练模型；调用predict方法测试模型，返回分类类标（1，-1）。</p><p>定义plot_decision_regions类，将分类结果可视化。</p><pre><code class="python">import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom matplotlib.colors import ListedColormapclass AdalineGD(object):    def __init__(self, eta=0.01, n_iter=50, random_state=1):                self.eta = eta                self.n_iter = n_iter                self.random_state = random_state    def fit(self, X, y):        rgen = np.random.RandomState(self.random_state)                self.w_ = rgen.normal(loc=0.0, scale=0.01,size=1 + X.shape[1])                self.cost_ = []        for i in range(self.n_iter):                        net_input = self.net_input(X)                        output = self.activation(net_input)                        errors = (y - output)                        self.w_[1:] += self.eta * X.T.dot(errors)                        self.w_[0] += self.eta * errors.sum()                        cost = (errors**2).sum() / 2.0                        self.cost_.append(cost)                return self    def net_input(self, X):                &quot;&quot;&quot;Calculate net input&quot;&quot;&quot;                return np.dot(X, self.w_[1:]) + self.w_[0]    def activation(self,X):        return X    def predict(self, X):                &quot;&quot;&quot;Return class label after unit step&quot;&quot;&quot;                return np.where(self.activation(self.net_input(X)) &gt;= 0.0, 1, -1)def plot_decision_regions(X, y, classifier, resolution=0.02):    # setup marker generator and color map        markers = (&#39;s&#39;, &#39;x&#39;, &#39;o&#39;, &#39;^&#39;, &#39;v&#39;)        colors = (&#39;red&#39;, &#39;blue&#39;, &#39;lightgreen&#39;, &#39;gray&#39;, &#39;cyan&#39;)        cmap = ListedColormap(colors[:len(np.unique(y))])    # plot the decision surface        x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1        x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1        xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),                                                      np.arange(x2_min, x2_max, resolution))        Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)        Z = Z.reshape(xx1.shape)        plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)        plt.xlim(xx1.min(), xx1.max())        plt.ylim(xx2.min(), xx2.max())    # plot class samples        for idx, cl in enumerate(np.unique(y)):                plt.scatter(x=X[y == cl, 0],                                        y=X[y == cl, 1],                                        alpha=0.8,                                        c=colors[idx],                                        marker=markers[idx],                                        label=cl,                                        edgecolor=&#39;black&#39;)</code></pre><p>测试选用不同学习率0.01和0.001，可以从下图看到，选用学习率0.01使得代价函数随着迭代次数的增加而增加，而选用学习率0.001使得代价函数逐渐收敛。在实际解决问题的过程中，选择过大的学习率可能会错过全局最优解，我们应该选择合适的学习率。</p><pre><code class="python">df = pd.read_csv(&#39;https://archive.ics.uci.edu/ml/&#39; ...              &#39;machine-learning-databases/iris/iris.data&#39;, ...               header=None)# select setosa and versicolor y = df.iloc[0:100, 4].values y = np.where(y == &#39;Iris-setosa&#39;, -1, 1)# extract sepal length and petal length X = df.iloc[0:100, [0, 2]].valuesfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))ada1 = AdalineGD(n_iter=10, eta=0.01).fit(X, y)ax[0].plot(range(1, len(ada1.cost_) + 1),np.log10(ada1.cost_), marker=&#39;o&#39;)ax[0].set_xlabel(&#39;Epochs&#39;)ax[0].set_ylabel(&#39;log(Sum-squared-error)&#39;)ax[0].set_title(&#39;Adaline - Learning rate 0.01&#39;)ada2 = AdalineGD(n_iter=10, eta=0.0001).fit(X, y)ax[1].plot(range(1, len(ada2.cost_) + 1),ada2.cost_, marker=&#39;o&#39;)ax[1].set_xlabel(&#39;Epochs&#39;)ax[1].set_ylabel(&#39;Sum-squared-error&#39;)ax[1].set_title(&#39;Adaline - Learning rate 0.0001&#39;)plt.show()</code></pre><p><img src="http://i1.fuimg.com/700703/e8188b7c3918646c.png" alt></p><p>为了优化算法性能，可以对数据做特征缩放，采用以下规则对数据进行特征缩放：<br>$$<br>x_{j}:=\frac{x_j-u_j}{\delta_j}<br>$$<br>$x_j$为训练样本n中第j个特征的所有值的向量，$u_j$、$\delta_j$分别是样本中第j个特征的平均值和标准差。</p><pre><code class="python">X_std = np.copy(X)X_std[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std()X_std[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std()ada = AdalineGD(n_iter=15, eta=0.01)ada.fit(X_std, y)plot_decision_regions(X_std, y, classifier=ada)plt.title(&#39;Adaline - Gradient Descent&#39;) plt.xlabel(&#39;sepal length [standardized]&#39;)plt.ylabel(&#39;petal length [standardized]&#39;)plt.legend(loc=&#39;upper left&#39;)plt.tight_layout()plt.show()plt.plot(range(1, len(ada.cost_) + 1), ada.cost_, marker=&#39;o&#39;)plt.xlabel(&#39;Epochs&#39;)plt.ylabel(&#39;Sum-squared-error&#39;)plt.show()</code></pre><img src="https://i.postimg.cc/L66YYhbG/2.png"><p><img src="https://i.postimg.cc/zDkHGT0n/3.png" alt></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>python机器学习分类算法--感知器</title>
      <link href="/2019/10/03/ganzhiji/"/>
      <url>/2019/10/03/ganzhiji/</url>
      
        <content type="html"><![CDATA[<h2 id="感知器基本原理"><a href="#感知器基本原理" class="headerlink" title="感知器基本原理"></a>感知器基本原理</h2><p>定义输入向量x，权值向量w，z是x与w的线性组合，z称作为净输入。</p><p>$$<br>x=\begin{bmatrix}x_{0}<br>\\ x_{1}<br>\\ …<br>\\ x_{m}<br>\end{bmatrix},x_{0}=1,w = \begin{bmatrix}w_{0}<br>\\ w_{1}<br>\\ …<br>\\ w_{m}<br>\end{bmatrix},z = x_{0}w_{0} + x_{1}w_{1} + … + x_{m}w_{m} = x^{T}w,<br>$$</p><p>定义激励函数h(z)，激励函数是一个分段函数，简单来说，当净输入z大于0时，将其划分到1类，否则为 -1类。</p><p>$$<br>h(z) = \begin{cases}<br>1 &amp; \text{ if } z\geq 0 \\<br>-1 &amp; \text{ if } z= else<br>\end{cases}<br>$$</p><p>感知器将输入值乘以权值得到净输入，通过激励函数将样本分为正负两类，感知器的工作过程如下：</p><ol><li>将权重初始化为零或一个极小的随机数。</li><li>迭代所有的训练样本x(i),执行以下操作：</li></ol><p>​    (1)计算输出值$\hat{y} $</p><p>​    (2)更新权重$w_{j} $<br>$$<br>w_{j}:=w_{j}+\Delta w_{j}\\ \Delta w_{j}=\eta(y^{(i)}-\hat{y}^{(i)})x_{j}^{(i)}<br>$$<br>$\eta$为学习率，0~1之间的数。</p><p>通过给每一个特征$x_i$分配对应权重$w_i$，将对应特征和权重相乘后求和得到$z$，然后根据激励函数将z划分为两类，若通过激励函数后所得的输出值（即预测值）与训练数据对应$y_i$相符合，则不改变权重w，若不相符，则改变权重w，使得重新分配的权重在下次分类更接近或者达到真实值。</p><p>需要注意的是，若样本线性不可分，权重w会不断更新，在实际代码实现的时候需要设定一个最大迭代次数。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>定义类Perceptron，创建对象时可选参数有eta即学习率$\eta$，n_iter为迭代次数，可调用fit方法训练模型，调用predict方法测试模型。</p><pre><code>import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom matplotlib.colors import ListedColormapclass Perceptron(object):    def __init__(self, eta=0.01, n_iter=50, random_state=1):                self.eta = eta                self.n_iter = n_iter                self.random_state = random_state    def fit(self, X, y):        rgen = np.random.RandomState(self.random_state)                self.w_ = rgen.normal(loc=0.0, scale=0.01,size=1 + X.shape[1])                self.errors_ = []        for _ in range(self.n_iter):                        errors = 0                        for xi, target in zip(X, y):                                update = self.eta * (target - self.predict(xi))                                self.w_[1:] += update * xi                                self.w_[0] += update                                errors += int(update != 0.0)                        self.errors_.append(errors)                return self    def net_input(self, X):                &quot;&quot;&quot;Calculate net input&quot;&quot;&quot;                return np.dot(X, self.w_[1:]) + self.w_[0]    def predict(self, X):                &quot;&quot;&quot;Return class label after unit step&quot;&quot;&quot;                return np.where(self.net_input(X) &gt;= 0.0, 1, -1)</code></pre><p>从pd库中调用数据集，下表为数据集的最后5条数据。</p><pre><code class="python">df = pd.read_csv(&#39;https://archive.ics.uci.edu/ml/&#39; ...              &#39;machine-learning-databases/iris/iris.data&#39;, ...               header=None) df.tail()</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th {    vertical-align: middle;}.dataframe thead th {    text-align: right;}</code></pre><p></style><p></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>0</th>      <th>1</th>      <th>2</th>      <th>3</th>      <th>4</th>    </tr>  </thead>  <tbody>    <tr>      <th>145</th>      <td>6.7</td>      <td>3.0</td>      <td>5.2</td>      <td>2.3</td>      <td>Iris-virginica</td>    </tr>    <tr>      <th>146</th>      <td>6.3</td>      <td>2.5</td>      <td>5.0</td>      <td>1.9</td>      <td>Iris-virginica</td>    </tr>    <tr>      <th>147</th>      <td>6.5</td>      <td>3.0</td>      <td>5.2</td>      <td>2.0</td>      <td>Iris-virginica</td>    </tr>    <tr>      <th>148</th>      <td>6.2</td>      <td>3.4</td>      <td>5.4</td>      <td>2.3</td>      <td>Iris-virginica</td>    </tr>    <tr>      <th>149</th>      <td>5.9</td>      <td>3.0</td>      <td>5.1</td>      <td>1.8</td>      <td>Iris-virginica</td>    </tr>  </tbody></table></div><p>使用matplotlib库将数据可视化。</p><pre><code class="python">import matplotlib.pyplot as plt# select setosa and versicolor y = df.iloc[0:100, 4].values y = np.where(y == &#39;Iris-setosa&#39;, -1, 1)# extract sepal length and petal length X = df.iloc[0:100, [0, 2]].values# plot data plt.scatter(X[:50, 0], X[:50, 1], label=&#39;setosa&#39;) plt.scatter(X[50:100, 0], X[50:100, 1],marker=&#39;x&#39;,label=&#39;versicolor&#39;)  plt.xlabel(&#39;sepal length [cm]&#39;) plt.ylabel(&#39;petal length [cm]&#39;)plt.legend(loc=&#39;upper left&#39;) plt.show() </code></pre><p><img src="http://i2.tiimg.com/700703/d147555495de8c6b.png" alt="散点图"></p><p>迭代次数和预测错误次数的折线图。</p><pre><code class="python">ppn = Perceptron(eta=0.1,n_iter=10)ppn.fit(X, y)plt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker=&#39;o&#39;) plt.xlabel(&#39;Epochs&#39;)plt.ylabel(&#39;Number of updates&#39;)plt.show()</code></pre><p><img src="http://i2.tiimg.com/700703/0f0a93745808decc.png" alt="折线图"></p><pre><code class="python">def plot_decision_regions(X, y, classifier, resolution=0.02):    # setup marker generator and color map        markers = (&#39;s&#39;, &#39;x&#39;, &#39;o&#39;, &#39;^&#39;, &#39;v&#39;)        colors = (&#39;red&#39;, &#39;blue&#39;, &#39;lightgreen&#39;, &#39;gray&#39;, &#39;cyan&#39;)        cmap = ListedColormap(colors[:len(np.unique(y))])    # plot the decision surface        x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1        x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1        xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),                                                      np.arange(x2_min, x2_max, resolution))        Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)        Z = Z.reshape(xx1.shape)        plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)        plt.xlim(xx1.min(), xx1.max())        plt.ylim(xx2.min(), xx2.max())    # plot class samples        for idx, cl in enumerate(np.unique(y)):                plt.scatter(x=X[y == cl, 0],                                        y=X[y == cl, 1],                                        alpha=0.8,                                        c=colors[idx],                                        marker=markers[idx],                                        label=cl,                                        edgecolor=&#39;black&#39;)plot_decision_regions(X, y, classifier=ppn)plt.xlabel(&#39;sepal length [cm]&#39;)plt.ylabel(&#39;petal length [cm]&#39;)plt.legend(loc=&#39;upper left&#39;)plt.show()</code></pre><p><img src="http://i2.tiimg.com/700703/15a977e327e47b82.png" alt="分类图"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>numpy_矩阵切片</title>
      <link href="/2019/10/03/numpy-array/"/>
      <url>/2019/10/03/numpy-array/</url>
      
        <content type="html"><![CDATA[<p>和python列表一样，numpy库中array也有着可切片的特性，这大大方便了我们对矩阵的可操作性。</p><pre><code class="python">import numpy as npa = np.arange(12)a.shape = (3,4)   #同写法a = a.reshape(3,4)print(a)</code></pre><p>首先我们现创建一个3*4的矩阵，如下：</p><pre><code>[[ 0  1  2  3] [ 4  5  6  7] [ 8  9 10 11]]</code></pre><p>可通过a[x,y]选择第x+1行第y+1列的元素</p><pre><code class="python">print(a[2,3]) #选择矩阵第3行第4列</code></pre><pre><code>11</code></pre><p>通过以下方法可对矩阵进行切割。</p><p>a[:,y]中“：”表示选择所有行，y表示选择第y+1列，a[:,1]表示选择矩阵a第2列的所有行，也就是[1 5 9]；</p><p>a[1,:]表示选择矩阵a第2行的所有列，也就是[4 5 6 7]。</p><p>a[:,:2]表示选择矩阵a第1和第2列的所有行。</p><pre><code class="python">print(a[:,1])  #选择矩阵第二列 print(a[1,:])  #选择矩阵第二行  print(a[:,:2]) #选择矩阵第一和第二列</code></pre><pre><code>[1 5 9][4 5 6 7][[0 1] [4 5] [8 9]]</code></pre><p>选择矩阵第2，3行和第2，3列</p><pre><code>print(a[1:,1:3]) </code></pre><pre><code>[[ 5  6] [ 9 10]]</code></pre><pre><code>print(a[:,[0,2]) #选择矩阵a的第1和第3列[[0 2] [4 6] [8 10]]</code></pre><p>用ravel()方法可将矩阵按行展开</p><pre><code class="python">print(a.ravel())#将矩阵转换成列表</code></pre><pre><code>[ 0  1  2  3  4  5  6  7  8  9 10 11]</code></pre><p>方法np.where(condition,x1,x2)，矩阵元素满足condition则选择用x1替换，否则用x2替换</p><pre><code class="python">y = np.array([&#39;one&#39;,&#39;zreo&#39;,&#39;one&#39;])y = np.where(y == &#39;one&#39;,1,0) #满足条件选1，不满足选0print(y)</code></pre><pre><code>[1 0 1]</code></pre>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>机器学习概述</title>
      <link href="/2019/10/02/intro-machinelearning/"/>
      <url>/2019/10/02/intro-machinelearning/</url>
      
        <content type="html"><![CDATA[<h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><p>近年来，NumPy、matplotlib和pandas等多种功能强大的开源库的出现，以及利用深度学习框架tensorflow等，使得机器学习这一领域绽放了无限的生机。</p><h3 id="机器学习的三种方法"><a href="#机器学习的三种方法" class="headerlink" title="机器学习的三种方法"></a>机器学习的三种方法</h3><ol><li><p>监督学习(supervised learning)</p><p>监督学习处理的问题分为两种，一是分类问题（classification），根据已经标记有是否为垃圾邮件的样本库，判断一封新邮件是否为垃圾邮件的问题。二是回归问题（regression），如我们想预测学生在SAT考试中数学科目的成绩，根据往期学生成绩与学习时间建立训练模型，通过学习时间预测学生的数学成绩。</p><p>监督学习是根据已有的样本训练模型，然后对未知的样本进行预测的一种方法。</p></li><li><p>非监督学习(unsupervised learning)</p><p>无监督学习是在没有已知输出变量和反馈函数指导的情况下提取有效信息来探索数据的整体结构的。通过非监督学习能够发现数据本身潜在的结构，在数据压缩中的降维邻域非监督学习也发挥着重要作用。</p></li><li><p>强化学习(reinforcement learning)</p><p>强化学习构建一个系统，在与环境的交互的过程中提高系统的性能。一个强化学习的经典例子就是象棋对弈游戏，系统根据当前局态（环境）决定落子的位置，游戏结束时胜负的判定可以作为反馈，通过这个反馈让系统做出调整，优化系统，提升系统的性能。</p></li></ol><h3 id="选择不同的算法训练模型"><a href="#选择不同的算法训练模型" class="headerlink" title="选择不同的算法训练模型"></a>选择不同的算法训练模型</h3><p>任何分类算法都有内在的局限性，在实际解决问题的过程中， 应该选用几种不同的算法来训练模型，并比较它们的性能，从中选择最优的一个。一个常用的性能指标是分类的准确性。</p><h3 id="机器学习技能树"><a href="#机器学习技能树" class="headerlink" title="机器学习技能树"></a>机器学习技能树</h3><p><img src="http://i1.fuimg.com/700703/cf43e5cf096624cd.png" alt="机器学习技能图谱"></p><p>图谱从<a href="https://github.com/TeamStuQ/skill-map转载" target="_blank" rel="noopener">https://github.com/TeamStuQ/skill-map转载</a></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>写在博客之前</title>
      <link href="/2019/10/02/whyblog/"/>
      <url>/2019/10/02/whyblog/</url>
      
        <content type="html"><![CDATA[<h2 id="Why-Blog？"><a href="#Why-Blog？" class="headerlink" title="Why Blog？"></a>Why Blog？</h2><blockquote><p>记录日常学习笔记。</p><p>方便快速复习已经学过的知识。</p><p>增加写文能力。</p><p>希望能帮助到其他人。</p></blockquote><h2 id="工匠精神"><a href="#工匠精神" class="headerlink" title="工匠精神"></a>工匠精神</h2><p>当今社会，经济和技术高速发展，身处于这样一个时代的我们难免有些浮躁。我所推崇的”工匠精神“，是一种刻骨钻研、追求创新的精神。我所希望的是，任何一种职业，不管在学习还是工作中，都应该像一名工匠一样，尽自己所能把事情做好，做精。</p>]]></content>
      
      
      <categories>
          
          <category> 杂谈 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 其他 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
